## Preface

æœ¬ä»“åº“è®°å½•å…³äºLLMå’ŒMultimodal LLMçš„æ–‡ç« ã€‚çœ‹è¿‡çš„æ–‡ç« ä¼šè‡³å°‘ç”¨ä¸€å¥è¯æ¦‚æ‹¬å†…å®¹ï¼Œæœ‰äº›è¿˜ä¼šæœ‰notesã€‚åªæœ‰æ ‡é¢˜çš„å°±æ˜¯è¿˜æ²¡çœ‹è¿‡çš„ï¼Œåªæ˜¯å…ˆå­˜æ¡£åˆ°è¿™é‡Œã€‚

æœ‰å…³OOD generalizationçš„paper listè¯·ç§»æ­¥ï¼š[link](https://github.com/NOVAglow646/OOD-Generalization-Paper-Reading-Notes)

###  ğŸ”¥ Updates

- 2024-05 æ¥ä¸‹æ¥ä¸»è¦å…³æ³¨æ¢ç©¶ICLæœºåˆ¶çš„ç›¸å…³å·¥ä½œ

## Directory

* [LLMs](#llms-large-language-models) 
* [MLLMs](multimodal-llms)
* [Vision-language understanding](#vision-language-understanding)
* [Unifying understanding and generation](#unifying-understanding-and-generation)
* [Prompt Learning](#prompt-learning)
* â­[In-Context Learning](#in-context-learning)
* [ICL Theories](#icl-theories)
* [Multimodal ICL](#multimodal-icl)



## LLMs (Large language models)

### 2024

1. **Model Editing with Canonical Examples** [[paper]](http://arxiv.org/abs/2402.06155) æå‡ºäº†ä¸€ä¸ªæ–°ä»»åŠ¡ï¼šè®©æ¨¡å‹å­¦ä¹ å‡ ä¸ªç‰¹å®šçš„æ–‡æœ¬ä¾‹å­ï¼Œä»¥å®ç°æŸäº›çº æ­£ï¼ŒåŒæ—¶è¿˜ä¸èƒ½è®©æ¨¡å‹æ”¹å˜å¾ˆå¤šã€‚
1. **Evaluating Large Language Models at Evaluating Instruction Following** [[paper]](https://openreview.net/forum?id=tr0KidwPLc) (ICLR 2024) 
1. **Not all Layers of LLMs are Necessary during Inference** (Arxiv April 2024) è®­ç»ƒä¸€ä¸ªå¯¹LLMä¸­é—´å±‚featureçš„åˆ†ç±»å™¨åˆ¤æ–­æ˜¯å¦åº”è¯¥æ—©åœæ¥è·å–æ—©åœå±‚æ•°ï¼Œæ¥åŠ é€ŸLLMæ¨ç†ã€‚è¿˜å‘ç°ä¸­é—´å±‚é¢„æµ‹çš„top probå’Œtop prob-second top probåœ¨å„ä¸ªä»»åŠ¡ä¸Šéƒ½å‘ˆç°å‡ºéšç€å±‚æ•°åŠ æ·±è€Œå¢åŠ å¹¶é€æ¸ç¨³å®šçš„è¶‹åŠ¿ï¼ˆä½†åœ¨ä¸åŒä»»åŠ¡ä¸Šå±‚æ•°ä¸ä¸€æ ·ï¼‰ã€‚[[paper]](http://arxiv.org/abs/2403.02181)
1. **Demonstrating Mutual Reinforcement Effect through Information Flow** (Arxiv March 2024) [[paper]](https://arxiv.org/pdf/2403.02902) ç ”ç©¶äº†åŒæ—¶è¿›è¡Œwordåˆ†ç±»å’Œtextåˆ†ç±»çš„MREï¼ˆMutual Reinforcement Effectï¼‰ä»»åŠ¡ï¼Œä¹Ÿè§‚å¯Ÿåˆ°äº†anchoré‚£ç¯‡ä¸­çš„ä¸‰ç§attention activationéšlayerçš„åˆ†å¸ƒè¶‹åŠ¿ã€‚
1. **A Theoretical Understanding of Self-Correction through In-context Alignment** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2405.18634) ç†è®ºåˆ†ætransformerä¸­çš„å„ä¸ªæ¨¡å—åœ¨self-correctionä¸­å‘æŒ¥çš„ä½œç”¨
1. **Mechanics of Next Token Prediction with Self-Attention** (AISTATS 2024) [[paper]](https://proceedings.mlr.press/v238/li24f.html) æ„é€ äº†ä¸€ä¸ªgraphæ¥æè¿°next token predictionä»»åŠ¡ï¼Œåœ¨ç®€åŒ–settingä¸‹ç†è®ºåˆ†æå‡ºlast tokenæ›´å€¾å‘äºç»™æ›´ç»å¸¸ä½œä¸ºlabelçš„tokenåˆ†é…æ›´é«˜çš„attentionã€‚
1. **The pitfalls of next-token prediction** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2403.06963) æŒ‡å‡ºäº†è‡ªå›å½’æ¨¡å‹çš„ç¼ºé™·ï¼šé”™è¯¯æ»šé›ªçƒæ•ˆåº”å’Œåœ¨ä¸€ä¸ªå•ä¸€tokenè·¯å¾„ä¸Šåªèƒ½å­¦å‡ºä¸€ä¸ªç±»ä¼¼induction headçš„shortcutæ¨¡å‹
1. **A Law of Next-Token Prediction in Large Language Models** (Arxiv August 2024) [[paper]](https://arxiv.org/pdf/2408.13442v1)
1. **The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning** (ICLR 2024) [[paper]](https://openreview.net/forum?id=wxJ0eXwwda) é€šè¿‡ICLï¼Œæ·»åŠ system promptå’Œé£æ ¼åŒ–çš„è¾“å‡ºï¼Œå®ç°åªç”¨å¾ˆå°‘çš„æ ·æœ¬ï¼ˆ3ä¸ªï¼‰æ¥æå‡LLM alignmentã€‚

### 2023

1. **Instruction-following Evaluation through Verbalizer Manipulation** (Arxiv July 2023) [[paper]](http://arxiv.org/abs/2307.10558) å‘ç°LLMéµå¾ªflipped-label instructionsçš„èƒ½åŠ›å¾ˆå·®ï¼Œè¯´æ˜ICLå¯èƒ½åªæ˜¯ç›´æ¥åˆ©ç”¨äº†é¢„è®­ç»ƒè¯­æ–™çš„çŸ¥è¯†ï¼Œè€Œä¸æ˜¯å­¦ä¹ äº†contextã€‚å³ä½¿æ˜¯å¼ºå¦‚GPT-4çš„æ¨¡å‹ä¹Ÿä¸èƒ½å¾ˆå¥½åœ°éµå¾ªflipped-label instructionsã€‚
2. **Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks** (Arxiv Aug 2023) [[paper]](http://arxiv.org/abs/2307.02477) ä¸€äº›ä¸»è¦å‘ç°ï¼šâ‘ æ¨¡å‹åœ¨counterfactualçš„settingä¸­æ€§èƒ½ä¼šå˜å·®ï¼Œä¸”settingå’Œå¸¸è§çš„ã€ç¬¦åˆäº‹å®çš„settingç›¸å·®è¶Šè¿œï¼Œæ€§èƒ½è¶Šå·®ï¼Œè¯´æ˜äº†æ¨¡å‹å¯èƒ½çš„è®°å¿†ç°è±¡ã€‚â‘¡åœ¨ç®—æœ¯ä»»åŠ¡ä¸Šï¼ŒICLèƒ½æå‡counterfactualï¼ˆä¸åŒè¿›åˆ¶çš„è®¡ç®—ï¼‰æ€§èƒ½ï¼Œä½†å’Œdefault settingçš„å·®è·éš¾ä»¥æŠ¹å¹³ã€‚
3. **Can the Inference Logic of Large Language Models be Disentangled into Symbolic Concepts?** (Arxiv Apr 2023) [[paper]](https://arxiv.org/abs/2304.01083) æå‡ºäº†ä¸€ç§empiricalçš„æŒ‡æ ‡æ¥è¡¡é‡è¾“å…¥å¥å­é‡Œçš„æŸäº›è¯å’Œè¯ç»„å¯¹æŸä¸€ç‰¹å®šè¾“å‡ºçš„å†³å®šç¨‹åº¦ã€‚
4. **Contrastive Chain-of-Thought Prompting** (Arxiv Nov 2023) [[paper]](http://arxiv.org/abs/2311.09277) ä½¿ç”¨å¯¹æ¯”CoTï¼Œå³ä¸€ä¸ªæ­£ç¡®CoTæ­é…ä¸€ä¸ªé”™è¯¯CoTèƒ½ç›¸æ¯”å¸¸è§„çš„CoTå¸¦æ¥æå‡.

### 2022

1. **Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models** [[paper]](https://arxiv.org/pdf/2210.14199.pdf)

### 2021

**LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS** å°†å¯¹æ¨¡å‹æƒé‡çŸ©é˜µçš„æ›´æ–°é™åˆ¶ä¸ºä½ç§©çŸ©é˜µä¹˜ç§¯$BA$çš„å½¢å¼ï¼Œæå¤§å‡å°‘äº†pre-trained modelè¿ç§»åˆ°æ–°ä»»åŠ¡çš„ä»£ä»·ï¼ˆä¸ç”¨fine-tuneæ‰€æœ‰å‚æ•°ï¼‰ [[paper]](https://arxiv.org/abs/2106.09685)

### 2019

1. **Are Sixteen Heads Really Better than One?** (NeurIPS 2019) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html) åœ¨æŸäº›å±‚ä¸Šï¼Œåªç”¨ä¸€ä¸ªheadæ€§èƒ½ä¹Ÿèƒ½ä¿æŒä¸å˜ã€‚åŒæ—¶æå‡ºäº†ä½¿ç”¨attentionæ¢¯åº¦æ¥è¡¡é‡headçš„é‡è¦æ€§ï¼Œæå‡ºäº†å‰ªæç­–ç•¥ã€‚



## MLLMs (Multimodal LLMs)

### 2024

1. **VisionLLaMA: A Unified LLaMA Interface for Vision Tasks** (Arxiv Mar 2024) [[paper]](https://arxiv.org/pdf/2403.00522) Vision LLaMa



## Vision-language understanding

### 2024

1. **Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models** (NeurIPS 2024) [[paper]](http://arxiv.org/abs/2406.14852) 
   åœ¨ä¸‰ä¸ªåˆæˆçš„ç©ºé—´ç†è§£ä»»åŠ¡ä¸Šè¯„æµ‹LLMå’ŒLVMï¼Œä¸»è¦å‘ç°ï¼š1ï¼‰è¯¥ä»»åŠ¡çš„æ€»ä½“è¡¨ç°å¹¶ä¸å¥½ 2ï¼‰å¯¹äºVLMè€Œè¨€ï¼Œæ›´ä¾èµ–äºè¯­è¨€ä¿¡æ¯è€Œä¸æ˜¯è§†è§‰ä¿¡æ¯åšå†³ç­–ï¼Œå»æ‰/æ‰°ä¹±è§†è§‰ä¿¡æ¯ç”šè‡³ä¼šæœ‰æå‡ 3ï¼‰VLMä¸­çš„language encoderæ¯”åŒæ ·çš„å•ç‹¬LLMæ€§èƒ½æ›´å¥½ï¼Œè¯´æ˜å¤šæ¨¡æ€pretrainå¯¹äºlanguageæœ‰ç”¨
2. **Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?** (Arxiv Sep 2024) [[paper]](https://arxiv.org/abs/2406.02537) 
3. **TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners** (Arxiv June 2024) [[paper]](http://arxiv.org/abs/2406.02537) æäº†ä¸€ä¸ªæ–°çš„ä¿¯è§†å›¾ç†è§£çš„æ•°æ®é›†ï¼Œå‘ç°VLMçš„ä¿¯è§†å›¾ç†è§£èƒ½åŠ›ä»ç„¶å¾ˆå·®
4. **Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models** [[paper]](https://openreview.net/pdf?id=nFU4xCyoe0) åŸå­è§†è§‰ä»»åŠ¡benchmark Atomic Visual Skills Benchmark (AVSBench) 
5. **Task Navigator: Decomposing Complex Tasks for Multimodal Large Language Models** (CVPR 2024) [[paper]](https://openaccess.thecvf.com/content/CVPR2024W/MAR/papers/Ma_Task_Navigator_Decomposing_Complex_Tasks_for_Multimodal_Large_Language_Models_CVPRW_2024_paper.pdf) å·¥ç¨‹æ–‡ç« ï¼Œå€ŸåŠ©LLMåˆ†è§£ä»»åŠ¡ï¼Œæå‡MLLMå®Œæˆå¤æ‚è§†è§‰ç†è§£ä»»åŠ¡çš„èƒ½åŠ›
6. **DOES SPATIAL COGNITION EMERGE IN FRONTIER MODELS? **(Arxiv Oct 2024) [[paper]](http://arxiv.org/abs/2410.06468) ç©ºé—´ç†è§£ä»»åŠ¡ SPACE benchmark
7. **Multimodal Chain-of-Thought Reasoning in Language Models** (TMLR 2024) [[paper]](http://arxiv.org/abs/2302.00923) ä¸¤é˜¶æ®µè®­ç»ƒï¼Œç¬¬ä¸€é˜¶æ®µæ¥å—æ–‡æœ¬å’Œè§†è§‰çš„èåˆç‰¹å¾è¾“å‡ºä¸€ä¸ªrationaleï¼ˆæ¨ç†è¿‡ç¨‹çš„æ–‡æœ¬æè¿°ï¼‰ï¼Œç¬¬äºŒé˜¶æ®µå°†ç”Ÿæˆçš„rationaleå’ŒåŸå§‹æ–‡æœ¬ç»“åˆï¼Œå†ä¸è§†è§‰ç‰¹å¾èåˆé‡æ–°è¾“å…¥æ¨¡å‹äº§ç”Ÿé¢„æµ‹ã€‚ã€insight 1ã€‘ç›´æ¥



## Unifying understanding and generation

### 2024

1. **Emu3: Next-Token Prediction is All You Need** (Arxiv September 2024) [[paper]](http://arxiv.org/abs/2409.18869) å°†æ–‡æœ¬ã€å›¾ç‰‡ã€è§†é¢‘éƒ½è½¬åŒ–ä¸ºtokenï¼Œè¿›è¡Œnext-token predictionçš„é¢„è®­ç»ƒã€‚èƒ½åŒæ—¶åšå›¾ç‰‡è§†é¢‘çš„ç”Ÿæˆã€è§†è§‰-è¯­è¨€ç†è§£ã€‚
2. **Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation** (Arxiv Oct 2024) [[paper]](http://arxiv.org/abs/2410.13848) ç”¨ä¸€ä¸ªè‡ªå›å½’transformerç»Ÿä¸€å®ç°å¤šæ¨¡æ€çš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡



## Test-time computation

### 2024

1. **Scaling LLM Test-time Compute Optimally can be More Effective than Scaling Model Parameters**  [[paper]](https://arxiv.org/pdf/2408.03314)
2. 



## Prompt Learning

### Prompt learningï¼š

1. **Conditional Prompt Learning for Vision-Language Models** (CoCoOp, CVPR2022) å°†å›¾ç‰‡ç‰¹å¾ç›´æ¥åŠ åˆ°context tokenä¸Šï¼Œè·å¾—sample-wiseçš„promptï¼Œä»¥å®ç°instanceçš„generalizationã€‚å…¶å®å°±æ˜¯å¸Œæœ›é€šè¿‡å¼•å…¥å›¾åƒä¿¡æ¯æ¥ä½¿å¾—promptæè¿°å¾—æ›´è´´åˆ‡ã€‚ä¸è¿‡æ„Ÿè§‰è¿˜æ˜¯æœ‰ç‚¹æ€ªï¼Œå› ä¸ºæ‰€æœ‰classéƒ½åŠ ä¸Šäº†åŒæ ·çš„å¯å­¦ä¹ prefixï¼Œä¸ºä»€ä¹ˆèƒ½æé«˜é¢„æµ‹ä¸ºæ­£ç¡®ç±»çš„æ¦‚ç‡ï¼Ÿ
2. MaPLe: Multi-modal Prompt Learning, CVPR2023 
3. Prompt-aligned Gradient for Prompt Tuning, ICCV2023
4. Compound Text-Guided Prompt Tuning via Image-Adaptive Cues, AAAI2024
5. MmAP : Multi-modal Alignment Prompt for Cross-domain Multi-task Learning, AAAI2024
6. **Improving Zero-Shot Generalization for CLIP with Synthesized Prompts** (ICCV 2023)

### For DA:

1. Domain Adaptation via Prompt Learning, arxiv 2022
2. AD-CLIP: Adapting Domains in Prompt Space Using CLIP, ICCV2023
3. Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation, NIPS2023
4. Prompt-based Distribution Alignment for Unsupervised Domain Adaptation, AAAI2024

### For DG:

1. StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization, arxiv2023





## In-Context Learning

### 2024

1. **Explore Spurious Correlations at the Concept Level in Language Models for Text Classification** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2311.08648) å‘ç°äº†LLMåœ¨æ–‡æœ¬åˆ†ç±»ä¸­ä¼šä¾èµ–çš„concept-label spurious correlationï¼Œæå‡ºä½¿ç”¨ChatGPTæ¥æ‰©å……æ•°æ®æ¥æ¶ˆé™¤è™šå‡å…³è”ã€‚

2. **Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes** (ongoing work) [[paper]](Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes) å‘ç°æ¨¡å‹å¯¹äºdemonstrationçš„permutation invarianceæˆ–è®¸æ˜¯ICL OODçš„å…³é”®ã€‚æå‡ºä½¿ç”¨ç›¸åŒçš„positional encodingæ¥æå‡ICL OODæ€§èƒ½ã€‚

3. **Simple synthetic data reduces sycophancy in large language models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2308.03958) LLMsä¼šè¿åˆæé—®è€…çš„è§‚ç‚¹è€Œç½”é¡¾äº‹å®ã€‚æå‡ºåˆæˆä¸€äº›ç”¨æˆ·çš„è§‚ç‚¹å’Œæ­£ç¡®æ€§æ— å…³çš„æ–°promptï¼Œç„¶ååœ¨è¿™äº›æ•°æ®ä¸Šfine-tuneæ¥è§£å†³sycophancyé—®é¢˜ã€‚

4. **Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions** (ICLR 2024 Oral) [[paper]](https://arxiv.org/abs/2310.03016) æ¢ç©¶transformeråœ¨ä¸€ç³»åˆ—ç¦»æ•£ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚ç‰¹åˆ«åœ°ï¼Œå‘ç°ç»è¿‡é¢„è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”éšæœºåˆå§‹åŒ–çš„æ¨¡å‹è·å¾—äº†æ›´å¼ºçš„æœ€è¿‘é‚»ã€disjunctionå’Œconjunctionçš„èƒ½åŠ›ã€‚

5. **Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning**  (Arxiv Jan 2024) å‘ç°ä½¿ç”¨batch ICLï¼Œå°†Nä¸ªexampleè®¾ç½®ä¸ºNä¸ªone-shot inferenceï¼Œå†æŠŠæ¯ä¸ªinferenceå¾—åˆ°çš„tokenåšå¹³å‡ï¼Œæ›¿æ¢åˆ°query sampleåšaggregationæœ€ç»ˆå†é¢„æµ‹èƒ½å¸¦æ¥æå‡ã€‚ä¸€ä¸ªå¥‡ç‰¹çš„å‘ç°æ˜¯åšaggregationæ—¶ä»æŸä¸€å±‚å¾€ååšæ€§èƒ½ä¼šçªå¢ï¼Œåœ¨é‚£ä¹‹å‰æ€§èƒ½æ¥è¿‘é›¶ã€‚å¯¹æ­¤è§£é‡Šæ˜¯transformerçš„ä½å±‚æ˜¯åœ¨å­¦è¯­ä¹‰ä¿¡æ¯ã€‚

6. **RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.13463) è¯„ä¼°æ¨¡å‹çš„æ”¹å˜å®ƒä»¬çš„åŸå§‹è¾“å‡ºå¹¶éµå¾ªå’Œä¸€å¼€å§‹ç›¸è¿èƒŒçš„æŒ‡ä»¤çš„èƒ½åŠ›ã€‚ä¸»è¦è§‚å¯Ÿï¼š1)å¤§éƒ¨åˆ†æ¨¡å‹éƒ½ä¼šå€¾å‘äºéµå®ˆå®ƒä»¬çš„é¢„è®­ç»ƒçŸ¥è¯† 2)æ¨¡å‹å¾ˆéš¾æ ¹æ®äººç±»åç»­çš„åé¦ˆæ³›åŒ–åˆ°æ–°çš„é—®é¢˜ 3)æ‰€æœ‰æ¨¡å‹éƒ½ä¼šé€æ­¥å¿˜è®°äººç±»åé¦ˆå¹¶è½å›åˆ°å®ƒä»¬çš„å†…éƒ¨çŸ¥è¯†é‡Œ 4)æ¨¡å‹æ˜¯ä¸æ˜¯ç¬¬ä¸€æ—¶é—´éµå®ˆäº†äººç±»çš„åé¦ˆï¼Œå¯¹äºåç»­çš„è¡Œä¸ºèµ·åˆ°å…³é”®ä½œç”¨

7. **Function Vectors in Large Language Models** (ICLR 2024) [[paper]](http://arxiv.org/abs/2310.15213) å‘ç°context promptçš„æœ€åä¸€ä¸ªtokençš„éšå±‚è¡¨ç¤ºencodeäº†è¿™ä¸ªä»»åŠ¡çš„ä¿¡æ¯ï¼Œç§°ä¸ºfunction vectorï¼ˆFVï¼‰ã€‚å°†å…¶åŠ åˆ°zero-shotçš„promptä¸Šï¼Œå‘ç°æœ‰æ˜¾è‘—æå‡ã€‚

8. **A Data Generation Perspective to the Mechanism of In-Context Learning** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.02212) æœ‰å…³task recognitionå’Œtask learningçš„ç»¼è¿°

9. **Identifying and Analyzing Task-Encoding Tokens in Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2401.11323) æ¢ç©¶äº†contextä¸­çš„templateè¯ï¼ˆ"data:","answer:"ï¼‰/stopwordï¼ˆæ ‡ç‚¹ã€è¿è¯ç­‰æ— æ„ä¹‰è¯ï¼‰/contentå¯¹performanceçš„æ„ä¹‰ã€‚ç»“æœå‘ç°templateè¯å¯¹ICLæ€§èƒ½æå‡æœ€æœ‰ç”¨ï¼Œcontentåè€Œæ²¡ä»€ä¹ˆç”¨ï¼›è¿˜æ¢ç©¶äº†templateè¯çš„ä»€ä¹ˆç‰¹å¾ä½¿å¾—å®ƒæœ‰åˆ«äºcontextä¸­çš„å…¶ä»–æˆåˆ†ï¼Œç»“æœå‘ç°templateè¯æœ¬èº«çš„è¯­ä¹‰ã€å…¶é‡å¤æ€§ã€å…¶åˆ†éš”xå’Œyçš„æ ¼å¼ä½œç”¨è¿™ä¸‰è€…éƒ½å¯¹ICLæ€§èƒ½æœ‰æ˜¾è‘—çš„ä½œç”¨ã€‚

10. **Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.19103) å‘ç°ï¼Œé—®é¢˜ä¸­çš„é”™è¯¯å‰æè€Œå¯¼è‡´çš„å›ç­”ä¸­çš„å¹»è§‰æ˜¯ç”±äºæ¨¡å‹ä¸­ç‰¹å®šçš„headçš„æ¿€æ´»æ‰€å¼•èµ·çš„ã€‚æå‡ºäº†ä¸€ç§å¼ºè¡Œæ¶ˆé™¤è¿™äº›headå¯¹äºé—®é¢˜ä¸­çš„é”™è¯¯å‰æå¯¹åº”çš„tokençš„attentionçš„æ–¹æ³•ã€‚

11. **In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering** (Arxiv Feb 2024) [[paper]](https://arxiv.org/abs/2311.06668) æå‡ºç”¨contextçš„ç¬¬Lå±‚è¡¨ç¤ºæ„é€ ä¸€ä¸ªè¡¨å¾ä»»åŠ¡ä¿¡æ¯çš„vectorï¼ˆICVï¼‰ï¼Œç„¶åå†åŠ åˆ°queryæ—¶çš„ç¬¬Lå±‚æ‰€æœ‰tokençš„è¡¨ç¤ºä¸Šã€‚

12. **The mechanistic basis of data dependence and abrupt learning in an in-context classification task** (ICLR 2024 Oral) [[paper]](https://arxiv.org/abs/2312.03002) æœ‰å…³transformer çš„IWLï¼ˆin-weights learningï¼‰å’ŒICLå­¦ä¹ è¿‡ç¨‹çš„å®éªŒæ€§åˆ†æã€‚åœ¨ä¸€ä¸ªä¸¤å±‚toy transformerä¸­æ­ç¤ºäº†induction headå­¦ä¹ æœºåˆ¶ã€‚

13. **Understanding In-context Learning From Repetitions** (ICLR 2024) [[paper]](https://openreview.net/forum?id=bGGYcvw8mp) æ­ç¤ºäº†contextä¸­é‡å¤å‡ºç°çš„patternä¼šå¯¼è‡´æ¨¡å‹æ›´å€¾å‘äºè¾“å‡ºè¿™ä¸ªpatternçš„ç°è±¡ã€‚

14. **In-context Learning Learns Label Relationships but is not Conventional Learning** (ICLR 2024) [[paper]](https://openreview.net/pdf?id=YPIA7bgd5y) ä»¥æ›´å¤§çš„æ¨¡å‹å’Œæ›´é•¿çš„contexté‡æ–°å®¡è§†ä»¥å¾€çš„ICLè®¨è®ºï¼Œå¹¶å¾—å‡ºäº†ä»¥ä¸‹ä¸‰ä¸ªç»“è®ºï¼š1)ICLä¼šå­¦x-yæ˜ å°„ï¼Œæ­£ç¡®çš„labelæ˜¯æœ‰ç”¨çš„ï¼Œä¸”æ¨¡å‹è¶Šå¤§è¿™ä¸€æ•ˆåº”è¶Šæ˜æ˜¾ 2)ICLèƒ½å­¦é¢„è®­ç»ƒæ—¶æ²¡è§è¿‡çš„æ–°ä»»åŠ¡ 3)å³ä½¿contextå¾ˆé•¿ï¼ŒICLä¹Ÿä¸èƒ½å½»åº•è¦†ç›–é¢„è®­ç»ƒè·å¾—çš„preference 4)LLMæ›´å…³æ³¨æ›´é è¿‘queryçš„example

15. **How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.02872) åœ¨ç®€å•çš„word classificationä»»åŠ¡ä¸Šï¼Œé¦–å…ˆæŒ‰ç…§ç±»ä¼¼Function Vectorçš„åšæ³•ï¼Œæå–å‡ºå¯¹è¾“å‡ºæ­£ç¡®é¢„æµ‹è´¡çŒ®æœ€å¤§çš„headã€‚ç„¶ååˆ†æè¿™äº›headå¹¶å‘ç°äº†å¦‚ä¸‹æœºåˆ¶ï¼šlabelçš„V encodeäº†labelçš„ç‰¹å¾ï¼Œlabelçš„K encodeäº†demonstrationçš„ç‰¹å¾ï¼›last tokençš„Q encodeäº†queryçš„ç‰¹å¾ï¼›last token queryå’Œæ­£ç¡®labelçš„Kçš„attention scoreæ¯”å…¶ä»–headçš„æ˜¾è‘—å¤§ï¼›last token Qä¸åœ¨contextä¸­å‡ºç°æ›´å¤šçš„label/æ›´é è¿‘queryçš„labelçš„Kçš„attention scoreæ›´å¤§ã€‚

16. **Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2312.12141) æå‡ºäº†ä¸€ç§å®šä½transformerä¸­å¯¹è¾“å‡ºæŸä¸€labelè´¡çŒ®æœ€å¤§çš„attentionæˆ–FFN layerï¼ˆæˆ–å…¶subvalueï¼‰çš„æ–¹æ³•ã€‚

17. **In-Context Learning State Vector with Inner and Momentum Optimization** (NeurIPS 2024) [[paper]](http://arxiv.org/abs/2404.11225) æäº†ä¸€ç§æ–°çš„ç”¨vectorå‹ç¼©ä¿¡æ¯çš„æŠ€æœ¯ï¼ˆState Vector SVï¼‰ï¼šæ˜¯å°†å‰Lå±‚çš„æ¯å±‚çš„attentionè¾“å‡ºconcatèµ·æ¥ã€‚ç„¶åæäº†ä¸‰ç§æŠ€æœ¯ï¼ˆaggregateæ¯ä¸€ä¸ªexampleçš„SVã€ç”¨momentumã€åˆ†ç»„æå–SVå†èšåˆï¼‰æ¥è¿›ä¸€æ­¥ä¼˜åŒ–SVï¼Œå–å¾—äº†ä¸€äº›æ€§èƒ½æå‡ã€‚

18. **GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network** (Arxiv Feb 2024)  [[paper]](http://arxiv.org/abs/2402.11709) æå‡ºå°†GNNæ’åœ¨LLMçš„æŸä¸€å±‚åé¢ï¼Œå¼ºè¡Œä½¿å¾—information flowï¼ˆtoken representationå°±æ˜¯node representationï¼‰æ˜¯ä»x->yå’Œy->:è¿è¾¹ï¼Œç„¶åå¾—åˆ°çš„node representationè¾“ç»™LLMçš„ä¸‹ä¸€å±‚ï¼ˆæ¯ä¸ªtokençš„éƒ½ä¿ç•™ç€ï¼Œå› ä¸ºGNNçš„è¾“å‡ºä¹Ÿæ˜¯æ‰€æœ‰nodeçš„è¾“å‡ºï¼‰ã€‚æœ€ååªåœ¨ICLæ•°æ®é›†ä¸Šå¾®è°ƒGNNï¼Œèƒ½å¤Ÿå®ç°å’Œloraåª²ç¾çš„é€Ÿåº¦å’Œæ›´å¥½çš„accã€‚

19. **Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2404.07546) å°†ICLèƒ½åŠ›åˆ†æˆ1)æ­£åˆ™åŒ–è¾“å‡ºçš„label spaceã€2)æ­£åˆ™åŒ–è¾“å‡ºçš„label formatï¼Œå’Œ3)æå‡label space/formatåˆ†å¸ƒå†…çš„åˆ¤åˆ«èƒ½åŠ›ä¸‰ä¸ªæ–¹é¢ã€‚ç»“è®ºï¼šICLçš„èƒ½åŠ›ä¸»è¦æ¥è‡ªå‰ä¸¤è€…ã€‚åŒæ—¶ä¹Ÿåœ¨å®éªŒä¸Šé—´æ¥è¯æ˜äº†ICLä¼šå€¾å‘äºé¢„æµ‹å‡ºcontextå’Œtestæ›´åƒçš„æ ·æœ¬çš„labelã€‚

20. **The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.11004) åœ¨é¢„æµ‹Markovåºåˆ—ä»»åŠ¡ä¸Šï¼Œæ­ç¤ºäº†å­˜åœ¨ä¸€ä¸ªå­¦ä¹ å‡ºä»ç®€å•åˆ°å¤æ‚functionçš„è¿‡ç¨‹ï¼ˆuniform -> unigram -> bigrams (optimal)ï¼‰ã€‚æ­¤å¤–ï¼Œä¹ŸéªŒè¯äº†ç±»ä¼¼retrievalï¼ˆn-gramï¼‰ï¼Œå³æ‰¾æœ€ç›¸ä¼¼çš„context tokenç„¶åå–å®ƒåé¢çš„tokenä½œä¸ºé¢„æµ‹çš„æœºåˆ¶ 

21. **In-Context Language Learning: Architectures and Algorithms** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2401.12973) æ„é€ äº†ä¸€ä¸ªæ¨¡æ‹Ÿçš„language token ICLä»»åŠ¡ï¼Œç»™äº†ä¸€ç³»åˆ—å®éªŒè¯æ®è¯´æ˜transformerå®ç°äº†å’Œn-gramç±»ä¼¼çš„retrievalè¿‡ç¨‹

22. **Trusting Your Evidence: Hallucinate Less with Context-aware Decoding** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2305.14739) ä¸ºäº†å¢å¼ºå¯¹contextçš„å…³æ³¨èƒ½åŠ›ï¼Œæå‡ºåœ¨æ¨ç†æ—¶åŠ æƒä»¥contextä¸ºæ¡ä»¶çš„é¢„æµ‹å’Œä¸å«contextçš„é¢„æµ‹ï¼š$y=\text{softmax}((1+\alpha) p_\theta(y|c,x)-\alpha p_\theta(y|x))$â€‹ ã€‚èƒŒåçš„ç†è®ºåŸºç¡€æ˜¯æœ´ç´ è´å¶æ–¯ [[blog]](https://spaces.ac.cn/archives/9617)

23. **How In-Context Learning Emerges from Training on Unstructured Data: On the Role of Co-Occurrence, Positional Information, and Noise Structures** (Arxiv Jun 2024) [[paper]](http://arxiv.org/abs/2406.00131) åœ¨éICLæ ¼å¼çš„æ•°æ®ä¸Šè®­ç»ƒï¼Œæ¢ç©¶äº†â€œå›½å®¶-é¦–éƒ½â€ç±»ä»»åŠ¡ï¼ˆé¢„è®­ç»ƒå¸¸è§ï¼‰å’Œè¾“å‡ºé¦–å­—æ¯ä»»åŠ¡ï¼ˆä¸å¸¸è§ï¼‰ï¼Œå‘ç°patternåœ¨è®­ç»ƒæ•°æ®é‡Œçš„é‡å¤æ€§å’Œä½ç½®ä¿¡æ¯åˆ†åˆ«æ˜¯è¿™ä¸¤ç§ä»»åŠ¡çš„å…³é”®ã€‚

24. **Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.00743) åˆ†æå¤šå±‚ã€PEã€multi headç­‰æ¨¡å—å¯¹äºæå‡ICLåœ¨çº¿æ€§å›å½’ä»»åŠ¡ä¸Šæ€§èƒ½çš„ä½œç”¨ã€‚

25. **Do pretrained Transformers Learn In-Context by Gradient Descent?** (ICML 2024) [[paper]](http://arxiv.org/abs/2310.08540) è®¨è®ºäº†ä¸€ä¸‹ç›®å‰ICLå·¥ä½œçš„ä¸åˆ‡å®é™…çš„settingï¼Œä»ä¸€äº›å®éªŒæŒ‡æ ‡ä¸Šè¯´æ˜äº†ICLå’ŒGDæœ‰æ˜¾è‘—ä¸åŒã€‚

26. **Rectifying Demonstration Shortcut in In-Context Learning** (NAACL 2024) [[paper]](http://arxiv.org/abs/2403.09488) å‘ç°contextå•è¯çš„å­—é¢æ„æ€ä¼šå½±å“ICLåˆ†ç±»çš„ç»“æœï¼ˆä¸€ç§shortcutï¼‰ã€‚æå‡ºäº†ä¸€ç§calibrationçš„ç­–ç•¥ã€‚

27. **Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning** (Arxiv June 2024) [[paper]](http://arxiv.org/abs/2406.14022) è®­ç»ƒè¿‡ç¨‹ä¸­task learningå’Œtask recognitionå­˜åœ¨ç«äº‰ç°è±¡

28. **Transformers Can Perform Distributionally-robust Optimisation through In-context Learning** (ICML 2024 workshop on ICL) [[paper]](https://openreview.net/pdf?id=MOgg2cEms5) ICLæœ‰ä¸€å®šçš„DROçš„èƒ½åŠ› 

29. **How Do In-Context Examples Affect Compositional Generalization?** (ACL 2024) [[paper]](http://arxiv.org/abs/2305.04835) å‘ç°context exampleå¯¹äºç»„åˆæ³›åŒ–èƒ½åŠ›å½±å“æ˜¾è‘—ã€‚å…·ä½“æ¥è¯´ï¼Œcontext exampleå’Œqueryè¶Šåƒã€exampleè¶Šå¤šæ ·ã€æ¯ä¸ªæ ·æœ¬è¶Šç®€å•ï¼Œæ³›åŒ–èƒ½åŠ›è¶Šå¥½ã€‚

30. **What Do Language Models Learn in Context? The Structured Task Hypothesis** (ACL 2024) [[paper]](http://arxiv.org/abs/2406.04216) é€šè¿‡å®éªŒéªŒè¯äº†ICLèƒ½å¤Ÿå¯¹é¢„è®­ç»ƒè§è¿‡çš„ä»»åŠ¡è¿›è¡Œå¤åˆçš„å‡è®¾ï¼Œå¦å®šäº†ICLä»…ä»…èƒ½å¤Ÿè¿›è¡Œåˆ†å¸ƒå†…ä»»åŠ¡çš„è¯•åˆ«ä»¥åŠICLèƒ½å¤Ÿæ³›åŒ–åˆ°æŸäº›è®­ç»ƒæ—¶æ²¡è§è¿‡çš„ä»»åŠ¡çš„å‡è®¾ã€‚

31. **What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation** (ICML 2024) [[paper]](http://arxiv.org/abs/2404.07129) è¯†åˆ«äº†transformeråœ¨è§£å†³ICLçš„copy-and-pasteä»»åŠ¡ä¸­å­˜åœ¨çš„ä¸‰ç§circuit

32. **In-Context Learning of Energy Functions** (ICML 2024 ICL workshop) [[paper]](http://arxiv.org/abs/2406.12785) æå‡ºäº†å°†next-tokençš„æ¡ä»¶åˆ†å¸ƒå»ºæ¨¡ä¸ºèƒ½é‡å‡½æ•°çš„å½¢å¼ï¼Œå‘ç°transformerä¹Ÿèƒ½åœ¨è¿™ç§å½¢å¼ä¸‹å±•ç°å‡ºICLèƒ½åŠ›

33. **From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2404.07544) å‘ç°è¯¸å¦‚GPT-4ï¼ŒClaude-3ä¹‹ç±»çš„LLMèƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹åšlinearå’Œnon-linear regressionï¼Œç”šè‡³æœ‰æ—¶èƒ½è¶…è¿‡supervised trainingçš„æ–¹æ³•ï¼ˆä½†ä»…é™äºå¾ˆå¤§çš„LLMï¼‰ã€‚

34. **Disentangling Latent Shifts of In-Context Learning Through Self-Training** (Arxiv Oct 2024) [[paper]](http://arxiv.org/abs/2410.01508) é’ˆå¯¹ICLä¸ç¨³å®šçš„é—®é¢˜ï¼Œæå‡ºä¸ºstudent LLMè®­ç»ƒä¸€ä¸ªadapterç”¨æ¥ä»teacher LLMé‚£é‡Œè·å–contextçš„çŸ¥è¯†ã€‚ã€insightã€‘è®¤ä¸ºä¹‹å‰çš„vectorç³»åˆ—å·¥ä½œåªè€ƒè™‘attn headï¼Œä¸å¤Ÿå…¨é¢ã€‚

35. **Learning Task Representations from In-Context Learning** (ICML 2024 ICL workshop) [[paper]](Learning Task Representations from In-Context Learning) æå‡ºlearnable task vectorï¼ˆLTVï¼‰ï¼Œä¸ºæ‰€æœ‰headå¢åŠ å¯å­¦ä¹ çš„æƒé‡ï¼Œç„¶ååŠ æƒç»„åˆæ¯ä¸€ä¸ªheadçš„activationæ¥å¾—åˆ°æ¯ä¸€å±‚function vectorã€‚å‘ç°å…¶å¯ä»¥å¢å¼ºICLçš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚

36. **Task Diversity Shortens the ICL Plateau** (Arxiv Oct 2024) [[paper]](http://arxiv.org/abs/2410.05448) synthetic settingï¼Œåœ¨æ›´å¤šçš„function classä¸Šè®­ç»ƒå¯ä»¥åŠ å¿«æ”¶æ•›ã€‚å‘ç°Aä»»åŠ¡è®­ç»ƒåˆ°lossæ­£åœ¨é€ƒç¦»plateauçš„checkpointåœ¨Bä»»åŠ¡ä¸Šç»§ç»­è®­ï¼Œå¯ä»¥åŠ å¿«Bçš„è®­ç»ƒï¼Œè¯´æ˜ä¸åŒä»»åŠ¡ä¹‹é—´æœ‰ä¸€äº›common structureï¼Œæä¾›äº†ä¸ºä»€ä¹ˆå¤šä»»åŠ¡è®­ç»ƒèƒ½æ›´å¿«æ”¶æ•›çš„ä¸€ä¸ªè§£é‡Šã€‚

37. **Many-Shot In-Context Learning** (ICML 2024 ICL workshop) [[paper]](http://arxiv.org/abs/2404.11018) ICLçš„æ½œåŠ›è¢«few-shoté™åˆ¶äº†

38. **Out-of-distribution generalization via composition: a lens through induction heads in Transformers** (Arxiv Aug 2024) [[papaer]](http://arxiv.org/abs/2408.09503) åœ¨OODçš„copyä»»åŠ¡ä¸Šï¼Œå‘ç°äº†OODæ€§èƒ½æºäºæ‰§è¡Œä¸åŒåŠŸèƒ½å±‚çš„compositionï¼ˆå¹¶æ²¡æœ‰æµ‹å¤æ‚çš„ç»„åˆæ³›åŒ–ä»»åŠ¡ï¼‰ã€‚è¿˜å‘ç°äº†induction headå’Œprevious token headçš„å„è‡ªå†…éƒ¨çš„è¡¨ç¤ºçš„ç›¸ä¼¼æ€§ã€‚

39. **Context-Scaling versus Task-Scaling in In-Context Learning** (Arxiv Oct 2024) [[paper]](https://arxiv.org/pdf/2410.12783) æ ¸å¿ƒå‘ç°ï¼škernel smoothingçš„ç‰¹å¾æ˜ å°„æ˜¯èƒ½å¤Ÿè¿›è¡Œcontext scalingçš„å…³é”®

40. **Bayesian scaling laws for in-context learning** (Arxiv Oct 2024) [[paper]](http://arxiv.org/abs/2410.16531) æ¨å¯¼äº†ä¸€ç§åŸºäºè´å¶æ–¯çš„scaling lawã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šæ•ˆæœæ¯”exponetial scaling lawå¥½ï¼Œåœ¨çœŸå®LLMå’Œæ•°æ®é›†ä¸Šæ•ˆæœè¿˜è¡Œã€‚

    



### 2023

1. **Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?** [[paper]](https://arxiv.org/abs/2202.12837) åšäº†ä¸€ç³»åˆ—æ¶ˆèå®éªŒæ¥å¯¹ICLè¿›è¡Œè§£é‡Šã€‚ä¸»è¦ç»“è®ºï¼šå³ä½¿inputå’Œlabelä¸æ˜¯ä¸€ä¸€å¯¹åº”ï¼Œåªè¦labelçš„åˆ†å¸ƒåˆç†ï¼Œé‚£ä¹ˆICLåŒæ ·èƒ½ç»™å‡ºè¾ƒä¸ºæ­£ç¡®çš„ç­”æ¡ˆ.
2. **Symbol tuning improves in-context learning in language models** (EMNLP 2023) [[paper]](http://arxiv.org/abs/2305.08298) å°†demonstrationçš„labelæ¢ä¸ºæ— æ„ä¹‰çš„symbolï¼Œç„¶åå¾®è°ƒï¼Œä»¥æ­¤å¼ºè¿«æ¨¡å‹å­¦ä¹ input-label mappingã€‚
3. **In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax** (Arxiv Nov 2023) [[paper] ](In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax) æœ¬æ–‡é€šè¿‡æ„å»ºä¸€äº›è¯­æ³•ä»»åŠ¡æ¥æµ‹è¯•æ¨¡å‹å¯¹äºå¥å­ç»“æ„çš„ç†è§£èƒ½åŠ›ï¼Œä»¥åŠOODæ³›åŒ–æ€§èƒ½ã€‚æ€»çš„è¯´æ¥ï¼ŒLLMè¿˜æ˜¯ä¼šç”¨åˆ°ä¸€äº›spurious correlationã€‚
4. **A Closer Look at In-Context Learning under Distribution Shifts** (Arxiv May 2023) [[paper]](http://arxiv.org/abs/2305.16704) åœ¨ä¸€å®šçš„åˆ†å¸ƒåç§»ä¸‹ï¼Œtransformeræ¯”set-based MLPçš„æ€§èƒ½å¥½ï¼›åœ¨ä¸¥é‡çš„åˆ†å¸ƒåç§»ä¸‹ï¼Œä¸¤ç§æ¨¡å‹çš„ICLèƒ½åŠ›éƒ½ä¸§å¤±äº†ã€‚
5. **Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation** (Arxiv May 2023) [[paper]](https://www.lsv.uni-saarland.de/wp-content/uploads/2023/07/Few-shot-Fine-tuning-vs.-In-context-Learning.pdf) åœ¨å‚æ•°é‡ç›¸å½“çš„æƒ…å†µä¸‹ï¼ŒICLçš„OODä¸å¦‚FTã€‚30Bçš„ICLè·Ÿ6.7Bçš„FTæ€§èƒ½ç›¸å½“ã€‚å¤§éƒ¨åˆ†æƒ…å†µä¸‹ICLä¸å¦‚FTã€‚
6. **Instruction-following Evaluation through Verbalizer Manipulation** (Arxiv July 2023) [[paper]](http://arxiv.org/abs/2307.10558) å‘ç°LLMéµå¾ªflipped-label instructionsçš„èƒ½åŠ›å¾ˆå·®ï¼Œè¯´æ˜ICLå¯èƒ½åªæ˜¯ç›´æ¥åˆ©ç”¨äº†é¢„è®­ç»ƒè¯­æ–™çš„çŸ¥è¯†ï¼Œè€Œä¸æ˜¯å­¦ä¹ äº†contextã€‚å³ä½¿æ˜¯å¼ºå¦‚GPT-4çš„æ¨¡å‹ä¹Ÿä¸èƒ½å¾ˆå¥½åœ°éµå¾ªflipped-label instructionsã€‚
7. **Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks** (Arxiv Aug 2023) [[paper]](http://arxiv.org/abs/2307.02477) ä¸€äº›ä¸»è¦å‘ç°ï¼šâ‘ æ¨¡å‹åœ¨counterfactualçš„settingä¸­æ€§èƒ½ä¼šå˜å·®ï¼Œä¸”settingå’Œå¸¸è§çš„ã€ç¬¦åˆäº‹å®çš„settingç›¸å·®è¶Šè¿œï¼Œæ€§èƒ½è¶Šå·®ï¼Œè¯´æ˜äº†æ¨¡å‹å¯èƒ½çš„è®°å¿†ç°è±¡ã€‚â‘¡åœ¨ç®—æœ¯ä»»åŠ¡ä¸Šï¼ŒICLèƒ½æå‡counterfactualï¼ˆä¸åŒè¿›åˆ¶çš„è®¡ç®—ï¼‰æ€§èƒ½ï¼Œä½†å’Œdefault settingçš„å·®è·éš¾ä»¥æŠ¹å¹³ã€‚
8. **What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning** (Findings of ACL 2023) [[paper]](http://arxiv.org/abs/2305.09731) åˆ†åˆ«ç”¨éšæœºlabelï¼ˆx-yæ˜ å°„å…³ç³»è¢«ç ´åï¼‰å’Œéè‡ªç„¶è¯­è¨€labelï¼ˆx-yæ˜ å°„å…³ç³»ä¿ç•™ï¼‰æ¥æ£€éªŒæ¨¡å‹çš„ä»é¢„è®­ç»ƒçŸ¥è¯†ä¸­è¯†åˆ«ä»»åŠ¡å’Œä»contextä¸­å­¦ä¹ input-labelæ˜ å°„å…³ç³»çš„èƒ½åŠ›ï¼Œå‘ç°ï¼šè¿™ä¸¤ç§èƒ½åŠ›åŒæ—¶å­˜åœ¨ï¼›ä»»åŠ¡è¯†åˆ«èƒ½åŠ›åŸºæœ¬ä¸éšæ¨¡å‹è§„æ¨¡å˜åŒ–ï¼›in-contextå­¦ä¹ èƒ½åŠ›ä¼šéšæ¨¡å‹å˜å¤§è€Œä¸Šå‡ã€‚
9. **Larger language models do in-context learning differently** (Arxiv Mar 2023) [[paper]](http://arxiv.org/abs/2303.03846) å’Œdisentanglement TR and TL é‚£ç¯‡å·®ä¸å¤šï¼Œå‘ç°äº†ï¼šå°æ¨¡å‹ä¼šå€¾å‘äºç”¨priorï¼Œéšç€æ¨¡å‹å¢å¤§ï¼Œè¦†ç›–priorè€Œä»contextå­¦ä¹ æ˜ å°„å…³ç³»çš„èƒ½åŠ›ä¼šè¶Šæ¥è¶Šå¼ºã€‚
10. **In-Context Learning Creates Task Vectors** (Arxiv Oct 2023) [[paper]](http://arxiv.org/abs/2310.15916) åŒæ ·å‘ç°contextçš„æœ€åä¸€ä¸ªtokençš„è¡¨ç¤ºencodeäº†è¯¥ä»»åŠ¡çš„ä¿¡æ¯ã€‚é€šè¿‡å®éªŒå‘ç°ICLè¿‘ä¼¼æ˜¯åœ¨å®ç°å¦‚ä¸‹è¿‡ç¨‹ï¼š1)ä»contextå­¦å‡ºä¸€ä¸ªæ˜ å°„å‡½æ•° 2)å°†è¿™ä¸ªæ˜ å°„å‡½æ•°ç”¨åˆ°queryä¸Šæ¥é¢„æµ‹ã€‚ä¸€ä¸ªé‡è¦è§‚å¯Ÿæ˜¯ï¼šè¯´æ˜æ¨¡å‹æ›´å€¾å‘äºä½¿ç”¨vectoré‡Œçš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åŸå§‹context
11. **Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning** (EMNLP 2023) [[paper]](http://arxiv.org/abs/2305.14160) æµ…å±‚ç½‘ç»œä»textåˆ°labelèšåˆä¿¡æ¯ï¼Œæ·±å±‚ç½‘ç»œä»labelåˆ°last tokenèšåˆä¿¡æ¯ã€‚
12. **Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models** (Arxiv Nov 2023) [[paper]](http://arxiv.org/abs/2311.00871) å‘ç°ICLåœ¨æµ‹è¯•å’Œé¢„è®­ç»ƒä»»åŠ¡ä¸ç›¸åŒæ—¶ï¼Œæ€§èƒ½ä¸å¥½ã€‚
13. **Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression** (NeurIPS 2023) [[paper]](https://arxiv.org/abs/2306.15063) å‘ç°é¢„è®­ç»ƒå­¦ä¹ çš„ä»»åŠ¡è¶Šå¤šï¼ŒICLåœ¨æ–°ä»»åŠ¡ä¸Šçš„æ³›åŒ–è¶Šå¼ºï¼ˆä¸åŒä»»åŠ¡ï¼šä¸åŒçº¿æ€§å›å½’çš„Wï¼‰
14. **The Transient Nature of Emergent In-Context Learning in Transformers** (NeurIPS 2023) [[paper]](http://arxiv.org/abs/2311.08360) è®­ç»ƒä»»åŠ¡ï¼šæ¯ä¸ªåºåˆ—çš„tokenéƒ½æœ‰ä¸€ä¸ªlabelã€‚è¯¥ä»»åŠ¡æ—¢å¯ä»¥ç”¨ICLè§£å†³ä¹Ÿå¯ä»¥ç”¨In-weights Learning (IWL)è§£å†³ã€‚å®éªŒå‘ç°éšç€è®­ç»ƒepochå¢åŠ ï¼ŒICLæ€§èƒ½å…ˆä¸Šå‡å†ä¸‹é™ï¼Œè€ŒIWLèƒ½åŠ›é€æ¸ä¸Šå‡ã€‚
15. **THE EFFECTS OF PRETRAINING TASK DIVERSITY ON IN-CONTEXT LEARNING OF RIDGE REGRESSION** (ICLR 2023 workshop) [[paper]](https://openreview.net/pdf?id=EshX_qlA3o) éšç€é¢„è®­ç»ƒæ—¶è§åˆ°çš„çº¿æ€§å›å½’wï¼ˆéƒ½æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼‰è¶Šæ¥è¶Šå¤šï¼ŒICLè¡¨ç°é€æ¸ä»MMSEï¼ˆé¢„è®­ç»ƒwçš„åŠ æƒç»„åˆï¼‰å˜ä¸ºå²­å›å½’ï¼ˆtestç†è®ºæœ€ä¼˜ï¼‰ã€‚
16. **Birth of a Transformer: A Memory Viewpoint** (NeurIPS 2023) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/0561738a239a995c8cd2ef0e50cfa4fd-Paper-Conference.pdf) æ„å»ºäº†ä¸€ä¸ªbigramä»»åŠ¡ï¼Œåœ¨ç®€åŒ–settingä¸‹æ¨å¯¼å‡ºäº†ä¸¤å±‚transformerè¦è§£å†³è¿™ä¸ªä»»åŠ¡æ‰€åº”å…·å¤‡çš„å‚æ•°é—­å¼è§£ï¼Œä»¥æ­¤è®¡ç®—æ¨¡å‹å‚æ•°å’Œæœ€ä¼˜è§£çš„å·®è·æ¥åˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„ICLèƒ½åŠ›çš„å˜åŒ–



### 2022

1. **What Can Transformers Learn In-Context? A Case Study of Simple Function Classes** (NeurIPS 2022) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/c529dba08a146ea8d6cf715ae8930cbe-Abstract-Conference.html) å®éªŒå‘ç°ï¼š1)linear functionæ˜¯èƒ½é€šè¿‡transformerå­¦åˆ°çš„ï¼ˆæ€§èƒ½èƒ½é€¼è¿‘æœ€å°äºŒä¹˜ä¼°è®¡ï¼‰2)ICLæœ‰ä¸€å®šçš„OODæ³›åŒ–èƒ½åŠ›ï¼ˆtrain -> test, context -> testï¼‰3)ICLä¹Ÿèƒ½å­¦åˆ°æ›´å¤æ‚çš„å‡½æ•°ï¼Œæ¯”å¦‚sparse linear functionsã€ReLU NNsã€decision treesã€‚
2. **Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?** (EMNLP 2022) [[paper]](http://arxiv.org/abs/2202.12837) æ¢ç©¶ICL workçš„å› ç´ ã€‚
3. **On the Compositional Generalization Gap of In-Context Learning** (Arxiv 2022) [[paper]](http://arxiv.org/abs/2211.08473) åœ¨CFQç­‰ç»„åˆæ³›åŒ–ä»»åŠ¡ä¸Šæµ‹ï¼Œå‘ç°å¤§æ¨¡å‹çš„OODï¼ˆqueryå’Œcontextä¸ä¸€è‡´ï¼‰å’ŒIDä¹‹é—´çš„ç»„åˆæ³›åŒ–èƒ½åŠ›çš„gapç›¸æ¯”å°æ¨¡å‹æ›´å°ã€‚



## ICL Theories

### 2024

1. **How do Transformers perform In-Context Autoregressive Learning?** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.05787) åœ¨é™å®šlinear attentionã€diagonal weight matrixç­‰æ¡ä»¶ä¸‹ï¼Œå¯¹äºåºåˆ—é¢„æµ‹ä»»åŠ¡$s_{T+1}=Ws_T$ï¼ˆæ–‡ç« è€ƒè™‘çš„$W$æ˜¯é…‰çŸ©é˜µå’Œæ­£äº¤çŸ©é˜µä¸¤ç§æƒ…å†µï¼‰ï¼Œä»ç†è®ºä¸Šç»™å‡ºäº†å–åˆ°å…¨å±€æœ€ä¼˜è§£æ—¶ï¼Œtransformer å‚æ•°æ‰€åº”æ»¡è¶³çš„æ€§è´¨ã€‚

2. **On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2405.16845) ç†è®ºè¯æ˜äº†ï¼Œä¸åŒäºç›´æ¥åœ¨ICLç›®æ ‡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç»è¿‡è‡ªå›å½’é¢„è®­ç»ƒçš„one-layer linear attentionä¸èƒ½åœ¨ç®€å•å¦‚æœä»é«˜æ–¯åˆ†å¸ƒçš„åºåˆ—ä¸Šå®ç°ICLã€‚

3. **How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?** (ICML 2024) [[paper]](http://arxiv.org/abs/2402.15607) åœ¨è¿›è¡ŒICLé¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œç»™å‡ºäº†éçº¿æ€§attentionçš„IDå’ŒOODçš„æ³›åŒ–ä¿è¯

4. **Why Larger Language Models Do In-context Learning Differently?** (ICML 2024) [[paper]](http://arxiv.org/abs/2405.19592) æœ¬æ–‡å¯¹äºæ›´å¤§çš„æ¨¡å‹æ›´å®¹æ˜“åœ¨flipped labelä»»åŠ¡ä¸Šå¤±è´¥ç»™äº†ç†è®ºè§£é‡Šï¼šå¤§æ¨¡å‹æ›´å®¹æ˜“å—åˆ°promptä¸­noiseçš„å½±å“ï¼Œè€Œå°æ¨¡å‹åªä¼šå…³æ³¨æ›´é‡è¦çš„featureæ‰€ä»¥ä¸å®¹æ˜“å—åˆ°noiseå½±å“ï¼Œè¿›è€Œä½¿pretrain featureå‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚

5. **Dual Operating Modes of In-Context Learning** (ICML 2024) [[paper]](http://arxiv.org/abs/2402.18819) ç†è®ºsettingï¼šåœ¨æ··åˆé«˜æ–¯çš„çº¿æ€§å›å½’ä¸Šé¢„è®­ç»ƒï¼Œåˆ†æäº†ç»™å®štest contextæ—¶çš„åéªŒæ¦‚ç‡ï¼Œè§£é‡Šäº†task recognitionå’Œtask learningï¼šå‘ç°contextè¾ƒçŸ­æ—¶ä»¥task recognitionï¼ˆè°ƒæ•´åéªŒçš„æ··åˆé«˜æ–¯çš„å„åˆ†é‡çš„æƒé‡ï¼‰ä¸ºä¸»ã€‚contextå˜é•¿ä¹‹åä»¥task learningä¸ºä¸»ã€‚

6. **In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2402.11639) softmaxèƒ½adaptivelyå­¦ä¸€ä¸ªattention windowæ¥å®ç°å°†context $y_i$ è¿›è¡Œæ’å€¼ä½œä¸ºé¢„æµ‹ï¼Œå°†åˆ†ç±»ä»»åŠ¡ä¸­è§åˆ°çš„retrievalæœºåˆ¶æ‹“å±•åˆ°äº†å›å½’ä»»åŠ¡ä¸Šã€‚

7. **Towards Better Understanding of In-Context Learning Ability from In-Context Uncertainty Quantification** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2405.15115) ç†è®ºï¼Œå¤šå¤´SoftMax attentionï¼Œä»»åŠ¡æ˜¯ä¼°è®¡p(y|x)å’ŒVar(y|x)ï¼Œç»™å‡ºäº†åˆ†å¸ƒå†…æ³›åŒ–error boundã€‚

8. **An Information-Theoretic Analysis of In-Context Learning** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2401.15530) åœ¨ä¿¡æ¯è®ºè§†è§’ä¸‹ï¼Œå°†ICLæ³›åŒ–è¯¯å·®æ‹†è§£ä¸ºå¤šé¡¹ã€‚

### 2023

1. **What learning algorithm is in-context learning? Investigations with linear models** (ICLR 2023) [[paper]](http://arxiv.org/abs/2211.15661) è¿˜æ²¡çœ‹ï¼Œç†è®ºç†è§£ICLæœºåˆ¶çš„æ–‡ç« ï¼Œlinear regressionä»»åŠ¡ï¼Œä½†å®ƒçš„ç†è®ºè®¾å®šæ˜¯æ¨¡å‹è¦åœ¨ICLä»»åŠ¡ä¸Šé¢„è®­ç»ƒï¼Œä¸å®é™…çš„Auto Regressiveé¢„è®­ç»ƒæœ‰è¾ƒå¤§gapã€‚å®ƒçš„è¯æ˜æ€è·¯ä¹Ÿæ˜¯é€šè¿‡ç½‘ç»œå‚æ•°æ„é€ è§£ï¼Œå’ŒA Theoretical Understanding of Self-Correction through In-context Alignmentè¿™ç¯‡ç±»ä¼¼ã€‚
2. **Transformers as Algorithms: Generalization and Stability in In-context Learning** (ICML 2023) [[paper]](http://arxiv.org/abs/2301.07067) è€ƒè™‘äº†contextä¸ºä¸€ç³»åˆ—ç‹¬ç«‹pairå’Œå‰åæ ·æœ¬æœ‰å…³è”ä¸¤ç§æ¨¡å¼ï¼Œåœ¨è¿›è¡ŒICLé¢„è®­ç»ƒçš„æ¡ä»¶ä¸‹ï¼Œç»™äº†ä¸€ä¸ªnon-linear transformerçš„excess riskçš„upper bound
3. **In-Context Convergence of Transformers** (Arxiv Oct 2023) [[paper]](http://arxiv.org/abs/2310.05249) linear regressionä»»åŠ¡ï¼Œéœ€è¦é¢„è®­ç»ƒï¼Œä¸€å±‚éçº¿æ€§attentionï¼Œä½†æ˜¯åšäº†å…¶ä»–ç®€åŒ–ä½¿å¾—transforerå°±æ˜¯åœ¨æ ¹æ®xä¹‹é—´çš„attention weightæ¥åŠ æƒç»„åˆå„ä¸ªcontext yä½œä¸ºæœ€ç»ˆé¢„æµ‹ã€‚
4. **Trained Transformers Learn Linear Models In-Context** (Arxiv Oct 2023) [[paper]](http://arxiv.org/abs/2306.09927) linear regressionä»»åŠ¡ï¼Œéœ€è¦é¢„è®­ç»ƒï¼Œä¸€å±‚çº¿æ€§attentionã€‚è¯æ˜äº†é¢„è®­ç»ƒlossæ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£æ—¶ï¼Œå½“è®­ç»ƒå’Œæµ‹è¯•contextè¶³å¤Ÿé•¿æ—¶ï¼Œèƒ½å­¦åˆ°æµ‹è¯•promptä¸Šçš„æ­£ç¡®è§£Wã€‚
5. **What and How Does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization** (Arxiv Oct 2023) [[paper]](arXiv:2305.19420) ç†è®ºæ–‡ç« ï¼Œè¿˜æ²¡çœ‹
6. **Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection** (NeurIPS 2023) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/b2e63e36c57e153b9015fece2352a9f9-Paper-Conference.pdf) è¯æ˜äº†å­˜åœ¨ä¸€ä¸ªL-å±‚çº¿æ€§transformeråœ¨çº¿æ€§å›å½’ã€lassoã€ridgeé—®é¢˜ä¸Šerroræœ‰ä¸Šç•Œã€‚åŒæ—¶åœ¨ç†è®ºå’Œå®éªŒä¸Šå‘ç°äº†ä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é¢„è®­ç»ƒçŸ¥è¯†çš„ç°è±¡ã€‚
7. **The Learnability of In-Context Learning** (NeurIPS 2023) [[paper]](https://openreview.net/forum?id=f3JNQd7CHM) è¯æ˜äº†å½“é¢„è®­ç»ƒåˆ†å¸ƒåŒ…å«ä¸‹æ¸¸ä»»åŠ¡çš„åˆ†å¸ƒçš„mixutureï¼ŒICLèƒ½é€¼è¿‘ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è´å¶æ–¯æœ€ä¼˜åˆ†ç±»å™¨ã€‚



## Multimodal ICL

### 2024

1. **What Makes Multimodal In-Context Learning Work?** (CVPR 2024 Workshop on Prompting in Vision) [[paper]](https://arxiv.org/abs/2404.15736) å¯¹Multimodal ICLçš„å®éªŒæ€§åˆ†æ
2. **Link-Context Learning for Multimodal LLMs** (CVPR 2024) [[paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html) æå‡ºä¸€ç§æ–°çš„fine-tune MLLMçš„æ–¹æ³•ï¼šè®©contextå’Œqueryå…·æœ‰ä¸€å®šçš„causalè”ç³»ï¼Œå‘ç°èƒ½æå‡æ¨¡å‹é€šè¿‡contextå­¦ä¹ æ–°æ¦‚å¿µçš„èƒ½åŠ›
3. **Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?** (Arxiv Sep 2024) 
4. **Finding Visual Task Vectors** (ECCV 2024) [[paper]](https://arxiv.org/pdf/2404.05729) 

### 2023

1. **What Makes Good Examples for Visual In-Context Learning?** [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/398ae57ed4fda79d0781c65c926d667b-Paper-Conference.pdf) çº¯vision ICLã€‚æ‰¾å’Œqueryæœ€ç›¸è¿‘çš„æ ·æœ¬æ¥åšICLï¼Œç±»ä¼¼Link-context learningã€‚

