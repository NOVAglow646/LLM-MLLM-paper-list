## Preface

æœ¬ä»“åº“è®°å½•å…³äºLLM (large language models)å’ŒVLM (vision-language models)çš„æ–‡ç« ï¼Œç‰¹åˆ«æ˜¯å…³äºIn-context Learning (ICL)çš„ã€‚çœ‹è¿‡çš„æ–‡ç« ä¼šè‡³å°‘ç”¨ä¸€å¥è¯æ¦‚æ‹¬å†…å®¹ï¼Œæœ‰äº›è¿˜ä¼šæœ‰notesã€‚åªæœ‰æ ‡é¢˜çš„å°±æ˜¯è¿˜æ²¡çœ‹è¿‡çš„ï¼Œåªæ˜¯å…ˆå­˜æ¡£åˆ°è¿™é‡Œã€‚

æœ‰å…³OOD generalizationçš„paper listè¯·ç§»æ­¥ï¼š[link](https://github.com/NOVAglow646/OOD-Generalization-Paper-Reading-Notes)

###  ğŸ”¥ Updates

- 2024-12 æ¥ä¸‹æ¥ä¸»è¦å…³æ³¨VLMçš„hallucinationã€reasoningé—®é¢˜ã€‚åŒæ—¶ä¹Ÿä¼šfollow ICLçš„æœ€æ–°è¿›å±•ã€‚
- 2024-05 æ¥ä¸‹æ¥ä¸»è¦å…³æ³¨æ¢ç©¶ICLæœºåˆ¶çš„ç›¸å…³å·¥ä½œ

## Directory

* [LLM](#llm) 
  * â­[In-Context Learning](#in-context-learning)
  * [ICL Theories](#icl-theories)
  * [Reasoning](#reasoning)
  * [Test-time compute](#test-time-compute)
  * [Alignment](#alignment)
  * [Interpretability](#interpretability)
  * [Other](#other)
* [VLM](#vlm)
  * [Evaluation and understandings of multimodal reasoning](#evaluation-and-understandings-of-multimodal-reasoning)
  * â­[Improving multimodal reasoning](#improving-multimodal-reasoning)
  * â­[Hallucination of VLMs](#hallucination-of-vlms)
  * [Explainability](#explainability)
  * [Unifying understanding and generation](#unifying-understanding-and-generation)
  * [Multimodal ICL](#multimodal-icl)
  * [Prompt Learning](#prompt-learning)

# LLM

## In-Context Learning

### 2024

1. **Explore Spurious Correlations at the Concept Level in Language Models for Text Classification** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2311.08648) å‘ç°äº†LLMåœ¨æ–‡æœ¬åˆ†ç±»ä¸­ä¼šä¾èµ–çš„concept-label spurious correlationï¼Œæå‡ºä½¿ç”¨ChatGPTæ¥æ‰©å……æ•°æ®æ¥æ¶ˆé™¤è™šå‡å…³è”ã€‚

2. **Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes** (ongoing work) [[paper]](Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes) å‘ç°æ¨¡å‹å¯¹äºdemonstrationçš„permutation invarianceæˆ–è®¸æ˜¯ICL OODçš„å…³é”®ã€‚æå‡ºä½¿ç”¨ç›¸åŒçš„positional encodingæ¥æå‡ICL OODæ€§èƒ½ã€‚

3. **Simple synthetic data reduces sycophancy in large language models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2308.03958) LLMsä¼šè¿åˆæé—®è€…çš„è§‚ç‚¹è€Œç½”é¡¾äº‹å®ã€‚æå‡ºåˆæˆä¸€äº›ç”¨æˆ·çš„è§‚ç‚¹å’Œæ­£ç¡®æ€§æ— å…³çš„æ–°promptï¼Œç„¶ååœ¨è¿™äº›æ•°æ®ä¸Šfine-tuneæ¥è§£å†³sycophancyé—®é¢˜ã€‚

4. **Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions** (ICLR 2024 Oral) [[paper]](https://arxiv.org/abs/2310.03016) æ¢ç©¶transformeråœ¨ä¸€ç³»åˆ—ç¦»æ•£ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚ç‰¹åˆ«åœ°ï¼Œå‘ç°ç»è¿‡é¢„è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”éšæœºåˆå§‹åŒ–çš„æ¨¡å‹è·å¾—äº†æ›´å¼ºçš„æœ€è¿‘é‚»ã€disjunctionå’Œconjunctionçš„èƒ½åŠ›ã€‚

5. **Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning**  (Arxiv Jan 2024) å‘ç°ä½¿ç”¨batch ICLï¼Œå°†Nä¸ªexampleè®¾ç½®ä¸ºNä¸ªone-shot inferenceï¼Œå†æŠŠæ¯ä¸ªinferenceå¾—åˆ°çš„tokenåšå¹³å‡ï¼Œæ›¿æ¢åˆ°query sampleåšaggregationæœ€ç»ˆå†é¢„æµ‹èƒ½å¸¦æ¥æå‡ã€‚ä¸€ä¸ªå¥‡ç‰¹çš„å‘ç°æ˜¯åšaggregationæ—¶ä»æŸä¸€å±‚å¾€ååšæ€§èƒ½ä¼šçªå¢ï¼Œåœ¨é‚£ä¹‹å‰æ€§èƒ½æ¥è¿‘é›¶ã€‚å¯¹æ­¤è§£é‡Šæ˜¯transformerçš„ä½å±‚æ˜¯åœ¨å­¦è¯­ä¹‰ä¿¡æ¯ã€‚

6. **RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.13463) è¯„ä¼°æ¨¡å‹çš„æ”¹å˜å®ƒä»¬çš„åŸå§‹è¾“å‡ºå¹¶éµå¾ªå’Œä¸€å¼€å§‹ç›¸è¿èƒŒçš„æŒ‡ä»¤çš„èƒ½åŠ›ã€‚ä¸»è¦è§‚å¯Ÿï¼š1)å¤§éƒ¨åˆ†æ¨¡å‹éƒ½ä¼šå€¾å‘äºéµå®ˆå®ƒä»¬çš„é¢„è®­ç»ƒçŸ¥è¯† 2)æ¨¡å‹å¾ˆéš¾æ ¹æ®äººç±»åç»­çš„åé¦ˆæ³›åŒ–åˆ°æ–°çš„é—®é¢˜ 3)æ‰€æœ‰æ¨¡å‹éƒ½ä¼šé€æ­¥å¿˜è®°äººç±»åé¦ˆå¹¶è½å›åˆ°å®ƒä»¬çš„å†…éƒ¨çŸ¥è¯†é‡Œ 4)æ¨¡å‹æ˜¯ä¸æ˜¯ç¬¬ä¸€æ—¶é—´éµå®ˆäº†äººç±»çš„åé¦ˆï¼Œå¯¹äºåç»­çš„è¡Œä¸ºèµ·åˆ°å…³é”®ä½œç”¨

7. **Function Vectors in Large Language Models** (ICLR 2024) [[paper]](http://arxiv.org/abs/2310.15213) å‘ç°context promptçš„æœ€åä¸€ä¸ªtokençš„éšå±‚è¡¨ç¤ºencodeäº†è¿™ä¸ªä»»åŠ¡çš„ä¿¡æ¯ï¼Œç§°ä¸ºfunction vectorï¼ˆFVï¼‰ã€‚å°†å…¶åŠ åˆ°zero-shotçš„promptä¸Šï¼Œå‘ç°æœ‰æ˜¾è‘—æå‡ã€‚5

8. **A Data Generation Perspective to the Mechanism of In-Context Learning** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.02212) æœ‰å…³task recognitionå’Œtask learningçš„ç»¼è¿°

9. **Identifying and Analyzing Task-Encoding Tokens in Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2401.11323) æ¢ç©¶äº†contextä¸­çš„templateè¯ï¼ˆ"data:","answer:"ï¼‰/stopwordï¼ˆæ ‡ç‚¹ã€è¿è¯ç­‰æ— æ„ä¹‰è¯ï¼‰/contentå¯¹performanceçš„æ„ä¹‰ã€‚ç»“æœå‘ç°templateè¯å¯¹ICLæ€§èƒ½æå‡æœ€æœ‰ç”¨ï¼Œcontentåè€Œæ²¡ä»€ä¹ˆç”¨ï¼›è¿˜æ¢ç©¶äº†templateè¯çš„ä»€ä¹ˆç‰¹å¾ä½¿å¾—å®ƒæœ‰åˆ«äºcontextä¸­çš„å…¶ä»–æˆåˆ†ï¼Œç»“æœå‘ç°templateè¯æœ¬èº«çš„è¯­ä¹‰ã€å…¶é‡å¤æ€§ã€å…¶åˆ†éš”xå’Œyçš„æ ¼å¼ä½œç”¨è¿™ä¸‰è€…éƒ½å¯¹ICLæ€§èƒ½æœ‰æ˜¾è‘—çš„ä½œç”¨ã€‚

10. **Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.19103) å‘ç°ï¼Œé—®é¢˜ä¸­çš„é”™è¯¯å‰æè€Œå¯¼è‡´çš„å›ç­”ä¸­çš„å¹»è§‰æ˜¯ç”±äºæ¨¡å‹ä¸­ç‰¹å®šçš„headçš„æ¿€æ´»æ‰€å¼•èµ·çš„ã€‚æå‡ºäº†ä¸€ç§å¼ºè¡Œæ¶ˆé™¤è¿™äº›headå¯¹äºé—®é¢˜ä¸­çš„é”™è¯¯å‰æå¯¹åº”çš„tokençš„attentionçš„æ–¹æ³•ã€‚

11. **In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering** (Arxiv Feb 2024) [[paper]](https://arxiv.org/abs/2311.06668) æå‡ºç”¨contextçš„ç¬¬Lå±‚è¡¨ç¤ºæ„é€ ä¸€ä¸ªè¡¨å¾ä»»åŠ¡ä¿¡æ¯çš„vectorï¼ˆICVï¼‰ï¼Œç„¶åå†åŠ åˆ°queryæ—¶çš„ç¬¬Lå±‚æ‰€æœ‰tokençš„è¡¨ç¤ºä¸Šã€‚

12. **The mechanistic basis of data dependence and abrupt learning in an in-context classification task** (ICLR 2024 Oral) [[paper]](https://arxiv.org/abs/2312.03002) æœ‰å…³transformer çš„IWLï¼ˆin-weights learningï¼‰å’ŒICLå­¦ä¹ è¿‡ç¨‹çš„å®éªŒæ€§åˆ†æã€‚åœ¨ä¸€ä¸ªä¸¤å±‚toy transformerä¸­æ­ç¤ºäº†induction headå­¦ä¹ æœºåˆ¶ã€‚

13. **Understanding In-context Learning From Repetitions** (ICLR 2024) [[paper]](https://openreview.net/forum?id=bGGYcvw8mp) æ­ç¤ºäº†contextä¸­é‡å¤å‡ºç°çš„patternä¼šå¯¼è‡´æ¨¡å‹æ›´å€¾å‘äºè¾“å‡ºè¿™ä¸ªpatternçš„ç°è±¡ã€‚

14. **In-context Learning Learns Label Relationships but is not Conventional Learning** (ICLR 2024) [[paper]](https://openreview.net/pdf?id=YPIA7bgd5y) ä»¥æ›´å¤§çš„æ¨¡å‹å’Œæ›´é•¿çš„contexté‡æ–°å®¡è§†ä»¥å¾€çš„ICLè®¨è®ºï¼Œå¹¶å¾—å‡ºäº†ä»¥ä¸‹ä¸‰ä¸ªç»“è®ºï¼š1)ICLä¼šå­¦x-yæ˜ å°„ï¼Œæ­£ç¡®çš„labelæ˜¯æœ‰ç”¨çš„ï¼Œä¸”æ¨¡å‹è¶Šå¤§è¿™ä¸€æ•ˆåº”è¶Šæ˜æ˜¾ 2)ICLèƒ½å­¦é¢„è®­ç»ƒæ—¶æ²¡è§è¿‡çš„æ–°ä»»åŠ¡ 3)å³ä½¿contextå¾ˆé•¿ï¼ŒICLä¹Ÿä¸èƒ½å½»åº•è¦†ç›–é¢„è®­ç»ƒè·å¾—çš„preference 4)LLMæ›´å…³æ³¨æ›´é è¿‘queryçš„example

15. **How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.02872) åœ¨ç®€å•çš„word classificationä»»åŠ¡ä¸Šï¼Œé¦–å…ˆæŒ‰ç…§ç±»ä¼¼Function Vectorçš„åšæ³•ï¼Œæå–å‡ºå¯¹è¾“å‡ºæ­£ç¡®é¢„æµ‹è´¡çŒ®æœ€å¤§çš„headã€‚ç„¶ååˆ†æè¿™äº›headå¹¶å‘ç°äº†å¦‚ä¸‹æœºåˆ¶ï¼šlabelçš„V encodeäº†labelçš„ç‰¹å¾ï¼Œlabelçš„K encodeäº†demonstrationçš„ç‰¹å¾ï¼›last tokençš„Q encodeäº†queryçš„ç‰¹å¾ï¼›last token queryå’Œæ­£ç¡®labelçš„Kçš„attention scoreæ¯”å…¶ä»–headçš„æ˜¾è‘—å¤§ï¼›last token Qä¸åœ¨contextä¸­å‡ºç°æ›´å¤šçš„label/æ›´é è¿‘queryçš„labelçš„Kçš„attention scoreæ›´å¤§ã€‚

16. **Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2312.12141) æå‡ºäº†ä¸€ç§å®šä½transformerä¸­å¯¹è¾“å‡ºæŸä¸€labelè´¡çŒ®æœ€å¤§çš„attentionæˆ–FFN layerï¼ˆæˆ–å…¶subvalueï¼‰çš„æ–¹æ³•ã€‚

17. **In-Context Learning State Vector with Inner and Momentum Optimization** (NeurIPS 2024) [[paper]](http://arxiv.org/abs/2404.11225) æäº†ä¸€ç§æ–°çš„ç”¨vectorå‹ç¼©ä¿¡æ¯çš„æŠ€æœ¯ï¼ˆState Vector SVï¼‰ï¼šæ˜¯å°†å‰Lå±‚çš„æ¯å±‚çš„attentionè¾“å‡ºconcatèµ·æ¥ã€‚ç„¶åæäº†ä¸‰ç§æŠ€æœ¯ï¼ˆaggregateæ¯ä¸€ä¸ªexampleçš„SVã€ç”¨momentumã€åˆ†ç»„æå–SVå†èšåˆï¼‰æ¥è¿›ä¸€æ­¥ä¼˜åŒ–SVï¼Œå–å¾—äº†ä¸€äº›æ€§èƒ½æå‡ã€‚

18. **GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network** (Arxiv Feb 2024)  [[paper]](http://arxiv.org/abs/2402.11709) æå‡ºå°†GNNæ’åœ¨LLMçš„æŸä¸€å±‚åé¢ï¼Œå¼ºè¡Œä½¿å¾—information flowï¼ˆtoken representationå°±æ˜¯node representationï¼‰æ˜¯ä»x->yå’Œy->:è¿è¾¹ï¼Œç„¶åå¾—åˆ°çš„node representationè¾“ç»™LLMçš„ä¸‹ä¸€å±‚ï¼ˆæ¯ä¸ªtokençš„éƒ½ä¿ç•™ç€ï¼Œå› ä¸ºGNNçš„è¾“å‡ºä¹Ÿæ˜¯æ‰€æœ‰nodeçš„è¾“å‡ºï¼‰ã€‚æœ€ååªåœ¨ICLæ•°æ®é›†ä¸Šå¾®è°ƒGNNï¼Œèƒ½å¤Ÿå®ç°å’Œloraåª²ç¾çš„é€Ÿåº¦å’Œæ›´å¥½çš„accã€‚

19. **Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2404.07546) å°†ICLèƒ½åŠ›åˆ†æˆ1)æ­£åˆ™åŒ–è¾“å‡ºçš„label spaceã€2)æ­£åˆ™åŒ–è¾“å‡ºçš„label formatï¼Œå’Œ3)æå‡label space/formatåˆ†å¸ƒå†…çš„åˆ¤åˆ«èƒ½åŠ›ä¸‰ä¸ªæ–¹é¢ã€‚ç»“è®ºï¼šICLçš„èƒ½åŠ›ä¸»è¦æ¥è‡ªå‰ä¸¤è€…ã€‚åŒæ—¶ä¹Ÿåœ¨å®éªŒä¸Šé—´æ¥è¯æ˜äº†ICLä¼šå€¾å‘äºé¢„æµ‹å‡ºcontextå’Œtestæ›´åƒçš„æ ·æœ¬çš„labelã€‚

20. **The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.11004) åœ¨é¢„æµ‹Markovåºåˆ—ä»»åŠ¡ä¸Šï¼Œæ­ç¤ºäº†å­˜åœ¨ä¸€ä¸ªå­¦ä¹ å‡ºä»ç®€å•åˆ°å¤æ‚functionçš„è¿‡ç¨‹ï¼ˆuniform -> unigram -> bigrams (optimal)ï¼‰ã€‚æ­¤å¤–ï¼Œä¹ŸéªŒè¯äº†ç±»ä¼¼retrievalï¼ˆn-gramï¼‰ï¼Œå³æ‰¾æœ€ç›¸ä¼¼çš„context tokenç„¶åå–å®ƒåé¢çš„tokenä½œä¸ºé¢„æµ‹çš„æœºåˆ¶ 

21. **In-Context Language Learning: Architectures and Algorithms** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2401.12973) æ„é€ äº†ä¸€ä¸ªæ¨¡æ‹Ÿçš„language token ICLä»»åŠ¡ï¼Œç»™äº†ä¸€ç³»åˆ—å®éªŒè¯æ®è¯´æ˜transformerå®ç°äº†å’Œn-gramç±»ä¼¼çš„retrievalè¿‡ç¨‹

22. **Trusting Your Evidence: Hallucinate Less with Context-aware Decoding** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2305.14739) ä¸ºäº†å¢å¼ºå¯¹contextçš„å…³æ³¨èƒ½åŠ›ï¼Œæå‡ºåœ¨æ¨ç†æ—¶åŠ æƒä»¥contextä¸ºæ¡ä»¶çš„é¢„æµ‹å’Œä¸å«contextçš„é¢„æµ‹ï¼š$y=\text{softmax}((1+\alpha) p_\theta(y|c,x)-\alpha p_\theta(y|x))$â€‹ ã€‚èƒŒåçš„ç†è®ºåŸºç¡€æ˜¯æœ´ç´ è´å¶æ–¯ [[blog]](https://spaces.ac.cn/archives/9617)

23. **How In-Context Learning Emerges from Training on Unstructured Data: On the Role of Co-Occurrence, Positional Information, and Noise Structures** (Arxiv Jun 2024) [[paper]](http://arxiv.org/abs/2406.00131) åœ¨éICLæ ¼å¼çš„æ•°æ®ä¸Šè®­ç»ƒï¼Œæ¢ç©¶äº†â€œå›½å®¶-é¦–éƒ½â€ç±»ä»»åŠ¡ï¼ˆé¢„è®­ç»ƒå¸¸è§ï¼‰å’Œè¾“å‡ºé¦–å­—æ¯ä»»åŠ¡ï¼ˆä¸å¸¸è§ï¼‰ï¼Œå‘ç°patternåœ¨è®­ç»ƒæ•°æ®é‡Œçš„é‡å¤æ€§å’Œä½ç½®ä¿¡æ¯åˆ†åˆ«æ˜¯è¿™ä¸¤ç§ä»»åŠ¡çš„å…³é”®ã€‚

24. **Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.00743) åˆ†æå¤šå±‚ã€PEã€multi headç­‰æ¨¡å—å¯¹äºæå‡ICLåœ¨çº¿æ€§å›å½’ä»»åŠ¡ä¸Šæ€§èƒ½çš„ä½œç”¨ã€‚

25. **Do pretrained Transformers Learn In-Context by Gradient Descent?** (ICML 2024) [[paper]](http://arxiv.org/abs/2310.08540) è®¨è®ºäº†ä¸€ä¸‹ç›®å‰ICLå·¥ä½œçš„ä¸åˆ‡å®é™…çš„settingï¼Œä»ä¸€äº›å®éªŒæŒ‡æ ‡ä¸Šè¯´æ˜äº†ICLå’ŒGDæœ‰æ˜¾è‘—ä¸åŒã€‚

26. **Rectifying Demonstration Shortcut in In-Context Learning** (NAACL 2024) [[paper]](http://arxiv.org/abs/2403.09488) å‘ç°contextå•è¯çš„å­—é¢æ„æ€ä¼šå½±å“ICLåˆ†ç±»çš„ç»“æœï¼ˆä¸€ç§shortcutï¼‰ã€‚æå‡ºäº†ä¸€ç§calibrationçš„ç­–ç•¥ã€‚

27. **Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning** (Arxiv June 2024) [[paper]](http://arxiv.org/abs/2406.14022) è®­ç»ƒè¿‡ç¨‹ä¸­task learningå’Œtask recognitionå­˜åœ¨ç«äº‰ç°è±¡

28. **Transformers Can Perform Distributionally-robust Optimisation through In-context Learning** (ICML 2024 workshop on ICL) [[paper]](https://openreview.net/pdf?id=MOgg2cEms5) ICLæœ‰ä¸€å®šçš„DROçš„èƒ½åŠ› 

29. **How Do In-Context Examples Affect Compositional Generalization?** (ACL 2024) [[paper]](http://arxiv.org/abs/2305.04835) å‘ç°context exampleå¯¹äºç»„åˆæ³›åŒ–èƒ½åŠ›å½±å“æ˜¾è‘—ã€‚å…·ä½“æ¥è¯´ï¼Œcontext exampleå’Œqueryè¶Šåƒã€exampleè¶Šå¤šæ ·ã€æ¯ä¸ªæ ·æœ¬è¶Šç®€å•ï¼Œæ³›åŒ–èƒ½åŠ›è¶Šå¥½ã€‚

30. **What Do Language Models Learn in Context? The Structured Task Hypothesis** (ACL 2024) [[paper]](http://arxiv.org/abs/2406.04216) é€šè¿‡å®éªŒéªŒè¯äº†ICLèƒ½å¤Ÿå¯¹é¢„è®­ç»ƒè§è¿‡çš„ä»»åŠ¡è¿›è¡Œå¤åˆçš„å‡è®¾ï¼Œå¦å®šäº†ICLä»…ä»…èƒ½å¤Ÿè¿›è¡Œåˆ†å¸ƒå†…ä»»åŠ¡çš„è¯•åˆ«ä»¥åŠICLèƒ½å¤Ÿæ³›åŒ–åˆ°æŸäº›è®­ç»ƒæ—¶æ²¡è§è¿‡çš„ä»»åŠ¡çš„å‡è®¾ã€‚

31. **What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation** (ICML 2024) [[paper]](http://arxiv.org/abs/2404.07129) è¯†åˆ«äº†transformeråœ¨è§£å†³ICLçš„copy-and-pasteä»»åŠ¡ä¸­å­˜åœ¨çš„ä¸‰ç§circuit

32. **In-Context Learning of Energy Functions** (ICML 2024 ICL workshop) [[paper]](http://arxiv.org/abs/2406.12785) æå‡ºäº†å°†next-tokençš„æ¡ä»¶åˆ†å¸ƒå»ºæ¨¡ä¸ºèƒ½é‡å‡½æ•°çš„å½¢å¼ï¼Œå‘ç°transformerä¹Ÿèƒ½åœ¨è¿™ç§å½¢å¼ä¸‹å±•ç°å‡ºICLèƒ½åŠ›

33. **From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2404.07544) å‘ç°è¯¸å¦‚GPT-4ï¼ŒClaude-3ä¹‹ç±»çš„LLMèƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹åšlinearå’Œnon-linear regressionï¼Œç”šè‡³æœ‰æ—¶èƒ½è¶…è¿‡supervised trainingçš„æ–¹æ³•ï¼ˆä½†ä»…é™äºå¾ˆå¤§çš„LLMï¼‰ã€‚

34. **Disentangling Latent Shifts of In-Context Learning Through Self-Training** (Arxiv Oct 2024) [[paper]](http://arxiv.org/abs/2410.01508) é’ˆå¯¹ICLä¸ç¨³å®šçš„é—®é¢˜ï¼Œæå‡ºä¸ºstudent LLMè®­ç»ƒä¸€ä¸ªadapterç”¨æ¥ä»teacher LLMé‚£é‡Œè·å–contextçš„çŸ¥è¯†ã€‚ã€insightã€‘è®¤ä¸ºä¹‹å‰çš„vectorç³»åˆ—å·¥ä½œåªè€ƒè™‘attn headï¼Œä¸å¤Ÿå…¨é¢ã€‚

35. **Learning Task Representations from In-Context Learning** (ICML 2024 ICL workshop) [[paper]](Learning Task Representations from In-Context Learning) æå‡ºlearnable task vectorï¼ˆLTVï¼‰ï¼Œä¸ºæ‰€æœ‰headå¢åŠ å¯å­¦ä¹ çš„æƒé‡ï¼Œç„¶ååŠ æƒç»„åˆæ¯ä¸€ä¸ªheadçš„activationæ¥å¾—åˆ°æ¯ä¸€å±‚function vectorã€‚å‘ç°å…¶å¯ä»¥å¢å¼ºICLçš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚

36. **Task Diversity Shortens the ICL Plateau** (Arxiv Oct 2024) [[paper]](http://arxiv.org/abs/2410.05448) synthetic settingï¼Œåœ¨æ›´å¤šçš„function classä¸Šè®­ç»ƒå¯ä»¥åŠ å¿«æ”¶æ•›ã€‚å‘ç°Aä»»åŠ¡è®­ç»ƒåˆ°lossæ­£åœ¨é€ƒç¦»plateauçš„checkpointåœ¨Bä»»åŠ¡ä¸Šç»§ç»­è®­ï¼Œå¯ä»¥åŠ å¿«Bçš„è®­ç»ƒï¼Œè¯´æ˜ä¸åŒä»»åŠ¡ä¹‹é—´æœ‰ä¸€äº›common structureï¼Œæä¾›äº†ä¸ºä»€ä¹ˆå¤šä»»åŠ¡è®­ç»ƒèƒ½æ›´å¿«æ”¶æ•›çš„ä¸€ä¸ªè§£é‡Šã€‚

37. **Many-Shot In-Context Learning** (ICML 2024 ICL workshop) [[paper]](http://arxiv.org/abs/2404.11018) ICLçš„æ½œåŠ›è¢«few-shoté™åˆ¶äº†

38. **Out-of-distribution generalization via composition: a lens through induction heads in Transformers** (Arxiv Aug 2024) [[papaer]](http://arxiv.org/abs/2408.09503) åœ¨OODçš„copyä»»åŠ¡ä¸Šï¼Œå‘ç°äº†OODæ€§èƒ½æºäºæ‰§è¡Œä¸åŒåŠŸèƒ½å±‚çš„compositionï¼ˆå¹¶æ²¡æœ‰æµ‹å¤æ‚çš„ç»„åˆæ³›åŒ–ä»»åŠ¡ï¼‰ã€‚è¿˜å‘ç°äº†induction headå’Œprevious token headçš„å„è‡ªå†…éƒ¨çš„è¡¨ç¤ºçš„ç›¸ä¼¼æ€§ã€‚

39. **Context-Scaling versus Task-Scaling in In-Context Learning** (Arxiv Oct 2024) [[paper]](https://arxiv.org/pdf/2410.12783) æ ¸å¿ƒå‘ç°ï¼škernel smoothingçš„ç‰¹å¾æ˜ å°„æ˜¯èƒ½å¤Ÿè¿›è¡Œcontext scalingçš„å…³é”®

40. **Bayesian scaling laws for in-context learning** (Arxiv Oct 2024) [[paper]](http://arxiv.org/abs/2410.16531) æ¨å¯¼äº†ä¸€ç§åŸºäºè´å¶æ–¯çš„scaling lawã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šæ•ˆæœæ¯”exponetial scaling lawå¥½ï¼Œåœ¨çœŸå®LLMå’Œæ•°æ®é›†ä¸Šæ•ˆæœè¿˜è¡Œã€‚

41. **Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks** (NeurIPS 2024) [[paper]](https://openreview.net/pdf/5737b58d308dafc16130635934df4276a7a574aa.pdf) æ¢ç©¶åœ¨modularåŠ æ³•é—®é¢˜ä¸Šçš„ICLçš„OODèƒ½åŠ›ï¼Œå¹¶è§£é‡Šäº†æ¨¡å‹ç»„ä»¶æ˜¯å¦‚ä½•å®ç°OODçš„èƒ½åŠ›çš„

42. **Improving In-Context Learning with Small Language Model Ensembles** (NeurIPS 2024 Workshop on Adaptive Foundation Models) [[paper]](http://arxiv.org/abs/2410.21868) å°†åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šfine-tuneçš„å¤šä¸ªå°æ¨¡å‹é¢„æµ‹çš„labelå’Œconfidenceä¸åŸå§‹labelç»„åˆåˆ°ä¸€èµ·ï¼Œå†è¾“ç»™å¤§æ¨¡å‹æ¥åšICLï¼Œå‘ç°å¯ä»¥æå‡æ€§èƒ½

43. **Algorithmic Phases of In-context Learning** (ICLR 2025 Ratings 10 8 6 6) [[paper]](https://openreview.net/pdf?id=XgH1wfHSX8) åœ¨ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ä¸Šï¼Œè¯†åˆ«äº†ICLçš„å››ç§æ¨ç†æ¨¡å¼ï¼šunigram/bigram-inference/retrievalï¼Œè¿™å‡ ç§æ¨¡å¼ä¹‹é—´çš„åˆ‡æ¢å¯ä»¥è§£é‡Šç›®å‰çš„ä¸€ç³»åˆ—ICLç°è±¡ï¼Œå¦‚task diversity threshold, transient nature, task retreival/task learning, early ascentç­‰ã€‚

44. **Can In-context Learning Really Generalize to Out-of-distribution Tasks?** (ICLR 2025 Ratings 8665) [[paper]](https://arxiv.org/abs/2410.09695) é€šè¿‡ä¸€ç³»åˆ—å®éªŒåˆ†æå‘ç°äº†ICLåœ¨OODä»»åŠ¡ä¸Šåªèƒ½å®ç°ä»é¢„è®­ç»ƒä»»åŠ¡ä¸­å¯»æ‰¾ä¸€ä¸ªæœ€ä¼˜ä»»åŠ¡æ¥æ‹Ÿåˆä¸‹æ¸¸ä»»åŠ¡ã€‚å¹¶ä»ç†è®ºä¸Šè®ºè¯äº†ICLçš„ç®—æ³•é€‰æ‹©æœºåˆ¶çš„å­˜åœ¨ã€‚

    



### 2023

1. **Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?** [[paper]](https://arxiv.org/abs/2202.12837) åšäº†ä¸€ç³»åˆ—æ¶ˆèå®éªŒæ¥å¯¹ICLè¿›è¡Œè§£é‡Šã€‚ä¸»è¦ç»“è®ºï¼šå³ä½¿inputå’Œlabelä¸æ˜¯ä¸€ä¸€å¯¹åº”ï¼Œåªè¦labelçš„åˆ†å¸ƒåˆç†ï¼Œé‚£ä¹ˆICLåŒæ ·èƒ½ç»™å‡ºè¾ƒä¸ºæ­£ç¡®çš„ç­”æ¡ˆ.
2. **Symbol tuning improves in-context learning in language models** (EMNLP 2023) [[paper]](http://arxiv.org/abs/2305.08298) å°†demonstrationçš„labelæ¢ä¸ºæ— æ„ä¹‰çš„symbolï¼Œç„¶åå¾®è°ƒï¼Œä»¥æ­¤å¼ºè¿«æ¨¡å‹å­¦ä¹ input-label mappingã€‚
3. **In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax** (Arxiv Nov 2023) [[paper] ](In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax) æœ¬æ–‡é€šè¿‡æ„å»ºä¸€äº›è¯­æ³•ä»»åŠ¡æ¥æµ‹è¯•æ¨¡å‹å¯¹äºå¥å­ç»“æ„çš„ç†è§£èƒ½åŠ›ï¼Œä»¥åŠOODæ³›åŒ–æ€§èƒ½ã€‚æ€»çš„è¯´æ¥ï¼ŒLLMè¿˜æ˜¯ä¼šç”¨åˆ°ä¸€äº›spurious correlationã€‚
4. **A Closer Look at In-Context Learning under Distribution Shifts** (Arxiv May 2023) [[paper]](http://arxiv.org/abs/2305.16704) åœ¨ä¸€å®šçš„åˆ†å¸ƒåç§»ä¸‹ï¼Œtransformeræ¯”set-based MLPçš„æ€§èƒ½å¥½ï¼›åœ¨ä¸¥é‡çš„åˆ†å¸ƒåç§»ä¸‹ï¼Œä¸¤ç§æ¨¡å‹çš„ICLèƒ½åŠ›éƒ½ä¸§å¤±äº†ã€‚
5. **Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation** (Arxiv May 2023) [[paper]](https://www.lsv.uni-saarland.de/wp-content/uploads/2023/07/Few-shot-Fine-tuning-vs.-In-context-Learning.pdf) åœ¨å‚æ•°é‡ç›¸å½“çš„æƒ…å†µä¸‹ï¼ŒICLçš„OODä¸å¦‚FTã€‚30Bçš„ICLè·Ÿ6.7Bçš„FTæ€§èƒ½ç›¸å½“ã€‚å¤§éƒ¨åˆ†æƒ…å†µä¸‹ICLä¸å¦‚FTã€‚
6. **Instruction-following Evaluation through Verbalizer Manipulation** (Arxiv July 2023) [[paper]](http://arxiv.org/abs/2307.10558) å‘ç°LLMéµå¾ªflipped-label instructionsçš„èƒ½åŠ›å¾ˆå·®ï¼Œè¯´æ˜ICLå¯èƒ½åªæ˜¯ç›´æ¥åˆ©ç”¨äº†é¢„è®­ç»ƒè¯­æ–™çš„çŸ¥è¯†ï¼Œè€Œä¸æ˜¯å­¦ä¹ äº†contextã€‚å³ä½¿æ˜¯å¼ºå¦‚GPT-4çš„æ¨¡å‹ä¹Ÿä¸èƒ½å¾ˆå¥½åœ°éµå¾ªflipped-label instructionsã€‚
7. **Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks** (Arxiv Aug 2023) [[paper]](http://arxiv.org/abs/2307.02477) ä¸€äº›ä¸»è¦å‘ç°ï¼šâ‘ æ¨¡å‹åœ¨counterfactualçš„settingä¸­æ€§èƒ½ä¼šå˜å·®ï¼Œä¸”settingå’Œå¸¸è§çš„ã€ç¬¦åˆäº‹å®çš„settingç›¸å·®è¶Šè¿œï¼Œæ€§èƒ½è¶Šå·®ï¼Œè¯´æ˜äº†æ¨¡å‹å¯èƒ½çš„è®°å¿†ç°è±¡ã€‚â‘¡åœ¨ç®—æœ¯ä»»åŠ¡ä¸Šï¼ŒICLèƒ½æå‡counterfactualï¼ˆä¸åŒè¿›åˆ¶çš„è®¡ç®—ï¼‰æ€§èƒ½ï¼Œä½†å’Œdefault settingçš„å·®è·éš¾ä»¥æŠ¹å¹³ã€‚
8. **What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning** (Findings of ACL 2023) [[paper]](http://arxiv.org/abs/2305.09731) åˆ†åˆ«ç”¨éšæœºlabelï¼ˆx-yæ˜ å°„å…³ç³»è¢«ç ´åï¼‰å’Œéè‡ªç„¶è¯­è¨€labelï¼ˆx-yæ˜ å°„å…³ç³»ä¿ç•™ï¼‰æ¥æ£€éªŒæ¨¡å‹çš„ä»é¢„è®­ç»ƒçŸ¥è¯†ä¸­è¯†åˆ«ä»»åŠ¡å’Œä»contextä¸­å­¦ä¹ input-labelæ˜ å°„å…³ç³»çš„èƒ½åŠ›ï¼Œå‘ç°ï¼šè¿™ä¸¤ç§èƒ½åŠ›åŒæ—¶å­˜åœ¨ï¼›ä»»åŠ¡è¯†åˆ«èƒ½åŠ›åŸºæœ¬ä¸éšæ¨¡å‹è§„æ¨¡å˜åŒ–ï¼›in-contextå­¦ä¹ èƒ½åŠ›ä¼šéšæ¨¡å‹å˜å¤§è€Œä¸Šå‡ã€‚
9. **Larger language models do in-context learning differently** (Arxiv Mar 2023) [[paper]](http://arxiv.org/abs/2303.03846) å’Œdisentanglement TR and TL é‚£ç¯‡å·®ä¸å¤šï¼Œå‘ç°äº†ï¼šå°æ¨¡å‹ä¼šå€¾å‘äºç”¨priorï¼Œéšç€æ¨¡å‹å¢å¤§ï¼Œè¦†ç›–priorè€Œä»contextå­¦ä¹ æ˜ å°„å…³ç³»çš„èƒ½åŠ›ä¼šè¶Šæ¥è¶Šå¼ºã€‚
10. **In-Context Learning Creates Task Vectors** (Arxiv Oct 2023) [[paper]](http://arxiv.org/abs/2310.15916) åŒæ ·å‘ç°contextçš„æœ€åä¸€ä¸ªtokençš„è¡¨ç¤ºencodeäº†è¯¥ä»»åŠ¡çš„ä¿¡æ¯ã€‚é€šè¿‡å®éªŒå‘ç°ICLè¿‘ä¼¼æ˜¯åœ¨å®ç°å¦‚ä¸‹è¿‡ç¨‹ï¼š1)ä»contextå­¦å‡ºä¸€ä¸ªæ˜ å°„å‡½æ•° 2)å°†è¿™ä¸ªæ˜ å°„å‡½æ•°ç”¨åˆ°queryä¸Šæ¥é¢„æµ‹ã€‚ä¸€ä¸ªé‡è¦è§‚å¯Ÿæ˜¯ï¼šè¯´æ˜æ¨¡å‹æ›´å€¾å‘äºä½¿ç”¨vectoré‡Œçš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åŸå§‹context
11. **Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning** (EMNLP 2023) [[paper]](http://arxiv.org/abs/2305.14160) æµ…å±‚ç½‘ç»œä»textåˆ°labelèšåˆä¿¡æ¯ï¼Œæ·±å±‚ç½‘ç»œä»labelåˆ°last tokenèšåˆä¿¡æ¯ã€‚
12. **Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models** (Arxiv Nov 2023) [[paper]](http://arxiv.org/abs/2311.00871) å‘ç°ICLåœ¨æµ‹è¯•å’Œé¢„è®­ç»ƒä»»åŠ¡ä¸ç›¸åŒæ—¶ï¼Œæ€§èƒ½ä¸å¥½ã€‚
13. **Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression** (NeurIPS 2023) [[paper]](https://arxiv.org/abs/2306.15063) å‘ç°é¢„è®­ç»ƒå­¦ä¹ çš„ä»»åŠ¡è¶Šå¤šï¼ŒICLåœ¨æ–°ä»»åŠ¡ä¸Šçš„æ³›åŒ–è¶Šå¼ºï¼ˆä¸åŒä»»åŠ¡ï¼šä¸åŒçº¿æ€§å›å½’çš„Wï¼‰
14. **The Transient Nature of Emergent In-Context Learning in Transformers** (NeurIPS 2023) [[paper]](http://arxiv.org/abs/2311.08360) è®­ç»ƒä»»åŠ¡ï¼šæ¯ä¸ªåºåˆ—çš„tokenéƒ½æœ‰ä¸€ä¸ªlabelã€‚è¯¥ä»»åŠ¡æ—¢å¯ä»¥ç”¨ICLè§£å†³ä¹Ÿå¯ä»¥ç”¨In-weights Learning (IWL)è§£å†³ã€‚å®éªŒå‘ç°éšç€è®­ç»ƒepochå¢åŠ ï¼ŒICLæ€§èƒ½å…ˆä¸Šå‡å†ä¸‹é™ï¼Œè€ŒIWLèƒ½åŠ›é€æ¸ä¸Šå‡ã€‚
15. **THE EFFECTS OF PRETRAINING TASK DIVERSITY ON IN-CONTEXT LEARNING OF RIDGE REGRESSION** (ICLR 2023 workshop) [[paper]](https://openreview.net/pdf?id=EshX_qlA3o) éšç€é¢„è®­ç»ƒæ—¶è§åˆ°çš„çº¿æ€§å›å½’wï¼ˆéƒ½æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼‰è¶Šæ¥è¶Šå¤šï¼ŒICLè¡¨ç°é€æ¸ä»MMSEï¼ˆé¢„è®­ç»ƒwçš„åŠ æƒç»„åˆï¼‰å˜ä¸ºå²­å›å½’ï¼ˆtestç†è®ºæœ€ä¼˜ï¼‰ã€‚
16. **Birth of a Transformer: A Memory Viewpoint** (NeurIPS 2023) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/0561738a239a995c8cd2ef0e50cfa4fd-Paper-Conference.pdf) æ„å»ºäº†ä¸€ä¸ªbigramä»»åŠ¡ï¼Œåœ¨ç®€åŒ–settingä¸‹æ¨å¯¼å‡ºäº†ä¸¤å±‚transformerè¦è§£å†³è¿™ä¸ªä»»åŠ¡æ‰€åº”å…·å¤‡çš„å‚æ•°é—­å¼è§£ï¼Œä»¥æ­¤è®¡ç®—æ¨¡å‹å‚æ•°å’Œæœ€ä¼˜è§£çš„å·®è·æ¥åˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„ICLèƒ½åŠ›çš„å˜åŒ–



### 2022

1. **What Can Transformers Learn In-Context? A Case Study of Simple Function Classes** (NeurIPS 2022) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/c529dba08a146ea8d6cf715ae8930cbe-Abstract-Conference.html) å®éªŒå‘ç°ï¼š1)linear functionæ˜¯èƒ½é€šè¿‡transformerå­¦åˆ°çš„ï¼ˆæ€§èƒ½èƒ½é€¼è¿‘æœ€å°äºŒä¹˜ä¼°è®¡ï¼‰2)ICLæœ‰ä¸€å®šçš„OODæ³›åŒ–èƒ½åŠ›ï¼ˆtrain -> test, context -> testï¼‰3)ICLä¹Ÿèƒ½å­¦åˆ°æ›´å¤æ‚çš„å‡½æ•°ï¼Œæ¯”å¦‚sparse linear functionsã€ReLU NNsã€decision treesã€‚
2. **Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?** (EMNLP 2022) [[paper]](http://arxiv.org/abs/2202.12837) æ¢ç©¶ICL workçš„å› ç´ ã€‚
3. **On the Compositional Generalization Gap of In-Context Learning** (Arxiv 2022) [[paper]](http://arxiv.org/abs/2211.08473) åœ¨CFQç­‰ç»„åˆæ³›åŒ–ä»»åŠ¡ä¸Šæµ‹ï¼Œå‘ç°å¤§æ¨¡å‹çš„OODï¼ˆqueryå’Œcontextä¸ä¸€è‡´ï¼‰å’ŒIDä¹‹é—´çš„ç»„åˆæ³›åŒ–èƒ½åŠ›çš„gapç›¸æ¯”å°æ¨¡å‹æ›´å°ã€‚



## ICL Theories

### 2024

1. **How do Transformers perform In-Context Autoregressive Learning?** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.05787) åœ¨é™å®šlinear attentionã€diagonal weight matrixç­‰æ¡ä»¶ä¸‹ï¼Œå¯¹äºåºåˆ—é¢„æµ‹ä»»åŠ¡$s_{T+1}=Ws_T$ï¼ˆæ–‡ç« è€ƒè™‘çš„$W$æ˜¯é…‰çŸ©é˜µå’Œæ­£äº¤çŸ©é˜µä¸¤ç§æƒ…å†µï¼‰ï¼Œä»ç†è®ºä¸Šç»™å‡ºäº†å–åˆ°å…¨å±€æœ€ä¼˜è§£æ—¶ï¼Œtransformer å‚æ•°æ‰€åº”æ»¡è¶³çš„æ€§è´¨ã€‚

2. **On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2405.16845) ç†è®ºè¯æ˜äº†ï¼Œä¸åŒäºç›´æ¥åœ¨ICLç›®æ ‡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç»è¿‡è‡ªå›å½’é¢„è®­ç»ƒçš„one-layer linear attentionä¸èƒ½åœ¨ç®€å•å¦‚æœä»é«˜æ–¯åˆ†å¸ƒçš„åºåˆ—ä¸Šå®ç°ICLã€‚

3. **How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?** (ICML 2024) [[paper]](http://arxiv.org/abs/2402.15607) åœ¨è¿›è¡ŒICLé¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œç»™å‡ºäº†éçº¿æ€§attentionçš„IDå’ŒOODçš„æ³›åŒ–ä¿è¯

4. **Why Larger Language Models Do In-context Learning Differently?** (ICML 2024) [[paper]](http://arxiv.org/abs/2405.19592) æœ¬æ–‡å¯¹äºæ›´å¤§çš„æ¨¡å‹æ›´å®¹æ˜“åœ¨flipped labelä»»åŠ¡ä¸Šå¤±è´¥ç»™äº†ç†è®ºè§£é‡Šï¼šå¤§æ¨¡å‹æ›´å®¹æ˜“å—åˆ°promptä¸­noiseçš„å½±å“ï¼Œè€Œå°æ¨¡å‹åªä¼šå…³æ³¨æ›´é‡è¦çš„featureæ‰€ä»¥ä¸å®¹æ˜“å—åˆ°noiseå½±å“ï¼Œè¿›è€Œä½¿pretrain featureå‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚

5. **Dual Operating Modes of In-Context Learning** (ICML 2024) [[paper]](http://arxiv.org/abs/2402.18819) ç†è®ºsettingï¼šåœ¨æ··åˆé«˜æ–¯çš„çº¿æ€§å›å½’ä¸Šé¢„è®­ç»ƒï¼Œåˆ†æäº†ç»™å®štest contextæ—¶çš„åéªŒæ¦‚ç‡ï¼Œè§£é‡Šäº†task recognitionå’Œtask learningï¼šå‘ç°contextè¾ƒçŸ­æ—¶ä»¥task recognitionï¼ˆè°ƒæ•´åéªŒçš„æ··åˆé«˜æ–¯çš„å„åˆ†é‡çš„æƒé‡ï¼‰ä¸ºä¸»ã€‚contextå˜é•¿ä¹‹åä»¥task learningä¸ºä¸»ã€‚

6. **In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2402.11639) softmaxèƒ½adaptivelyå­¦ä¸€ä¸ªattention windowæ¥å®ç°å°†context $y_i$ è¿›è¡Œæ’å€¼ä½œä¸ºé¢„æµ‹ï¼Œå°†åˆ†ç±»ä»»åŠ¡ä¸­è§åˆ°çš„retrievalæœºåˆ¶æ‹“å±•åˆ°äº†å›å½’ä»»åŠ¡ä¸Šã€‚

7. **Towards Better Understanding of In-Context Learning Ability from In-Context Uncertainty Quantification** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2405.15115) ç†è®ºï¼Œå¤šå¤´SoftMax attentionï¼Œä»»åŠ¡æ˜¯ä¼°è®¡p(y|x)å’ŒVar(y|x)ï¼Œç»™å‡ºäº†åˆ†å¸ƒå†…æ³›åŒ–error boundã€‚

8. **An Information-Theoretic Analysis of In-Context Learning** (Arxiv Jan 2024) [[paper]](http://arxiv.org/abs/2401.15530) åœ¨ä¿¡æ¯è®ºè§†è§’ä¸‹ï¼Œå°†ICLæ³›åŒ–è¯¯å·®æ‹†è§£ä¸ºå¤šé¡¹ã€‚

### 2023

1. **What learning algorithm is in-context learning? Investigations with linear models** (ICLR 2023) [[paper]](http://arxiv.org/abs/2211.15661) è¿˜æ²¡çœ‹ï¼Œç†è®ºç†è§£ICLæœºåˆ¶çš„æ–‡ç« ï¼Œlinear regressionä»»åŠ¡ï¼Œä½†å®ƒçš„ç†è®ºè®¾å®šæ˜¯æ¨¡å‹è¦åœ¨ICLä»»åŠ¡ä¸Šé¢„è®­ç»ƒï¼Œä¸å®é™…çš„Auto Regressiveé¢„è®­ç»ƒæœ‰è¾ƒå¤§gapã€‚å®ƒçš„è¯æ˜æ€è·¯ä¹Ÿæ˜¯é€šè¿‡ç½‘ç»œå‚æ•°æ„é€ è§£ï¼Œå’ŒA Theoretical Understanding of Self-Correction through In-context Alignmentè¿™ç¯‡ç±»ä¼¼ã€‚
2. **Transformers as Algorithms: Generalization and Stability in In-context Learning** (ICML 2023) [[paper]](http://arxiv.org/abs/2301.07067) è€ƒè™‘äº†contextä¸ºä¸€ç³»åˆ—ç‹¬ç«‹pairå’Œå‰åæ ·æœ¬æœ‰å…³è”ä¸¤ç§æ¨¡å¼ï¼Œåœ¨è¿›è¡ŒICLé¢„è®­ç»ƒçš„æ¡ä»¶ä¸‹ï¼Œç»™äº†ä¸€ä¸ªnon-linear transformerçš„excess riskçš„upper bound
3. **In-Context Convergence of Transformers** (Arxiv Oct 2023) [[paper]](http://arxiv.org/abs/2310.05249) linear regressionä»»åŠ¡ï¼Œéœ€è¦é¢„è®­ç»ƒï¼Œä¸€å±‚éçº¿æ€§attentionï¼Œä½†æ˜¯åšäº†å…¶ä»–ç®€åŒ–ä½¿å¾—transforerå°±æ˜¯åœ¨æ ¹æ®xä¹‹é—´çš„attention weightæ¥åŠ æƒç»„åˆå„ä¸ªcontext yä½œä¸ºæœ€ç»ˆé¢„æµ‹ã€‚
4. **Trained Transformers Learn Linear Models In-Context** (Arxiv Oct 2023) [[paper]](http://arxiv.org/abs/2306.09927) linear regressionä»»åŠ¡ï¼Œéœ€è¦é¢„è®­ç»ƒï¼Œä¸€å±‚çº¿æ€§attentionã€‚è¯æ˜äº†é¢„è®­ç»ƒlossæ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£æ—¶ï¼Œå½“è®­ç»ƒå’Œæµ‹è¯•contextè¶³å¤Ÿé•¿æ—¶ï¼Œèƒ½å­¦åˆ°æµ‹è¯•promptä¸Šçš„æ­£ç¡®è§£Wã€‚
5. **What and How Does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization** (Arxiv Oct 2023) [[paper]](arXiv:2305.19420) æ•°æ®ç”Ÿæˆæ¨¡å‹æ˜¯éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆå’ŒAn Explanation of In-context Learning as Implicit Bayesian Inferenceè¿™ç¯‡å¦‚å‡ºä¸€è¾™ï¼‰ï¼Œç†è®ºè¯æ˜äº†ICLèƒ½å…ˆæ ¹æ®contextæ¨æ–­ä¸€ä¸ªâ€œä»»åŠ¡æ¦‚å¿µâ€ $\theta$ï¼Œç„¶åæ ¹æ® $\theta$ ï¼Œqueryå’Œcontextæ¥æ¨æ–­yã€‚
6. **Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection** (NeurIPS 2023) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/b2e63e36c57e153b9015fece2352a9f9-Paper-Conference.pdf) è¯æ˜äº†å­˜åœ¨ä¸€ä¸ªL-å±‚çº¿æ€§transformeråœ¨çº¿æ€§å›å½’ã€lassoã€ridgeé—®é¢˜ä¸Šerroræœ‰ä¸Šç•Œã€‚åŒæ—¶åœ¨ç†è®ºå’Œå®éªŒä¸Šå‘ç°äº†ä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é¢„è®­ç»ƒçŸ¥è¯†çš„ç°è±¡ã€‚
7. **The Learnability of In-Context Learning** (NeurIPS 2023) [[paper]](https://openreview.net/forum?id=f3JNQd7CHM) è¯æ˜äº†å½“é¢„è®­ç»ƒåˆ†å¸ƒåŒ…å«ä¸‹æ¸¸ä»»åŠ¡çš„åˆ†å¸ƒçš„mixutureï¼ŒICLèƒ½é€¼è¿‘ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è´å¶æ–¯æœ€ä¼˜åˆ†ç±»å™¨ã€‚

### 2022

1. **An Explanation of In-context Learning as Implicit Bayesian Inference** (Arxiv 2022) [[paper]](https://arxiv.org/pdf/2111.02080) æ—©æœŸç»å…¸ä¹‹ä½œï¼Œéšé©¬å°”å¯å¤«æ¨¡å‹ï¼Œè¯æ˜ICLèƒ½å®ç°bayesian-optimal predictionã€‚



## Reasoning

### 2025

1. **Benchmarking and Understanding Compositional Relational Reasoning of LLMs** (AAAI 2025) [[paper]](http://arxiv.org/abs/2412.12841) æå‡ºäº†GAR benchmarkæ¥æµ‹è¯•æ¨¡å‹çš„Compositional Relational Reasoningèƒ½åŠ›ã€‚å‘ç°compositional gapéšç€æ¨¡å‹å¢å¤§è€Œå¢å¤§ã€‚åŒæ—¶å‘ç°äº†Vicunna-33bå­˜åœ¨ä¸€äº›å…±äº«çš„circuitèƒ½åœ¨ä¸åŒä»»åŠ¡ä¸­éƒ½å‘æŒ¥ä½œç”¨ã€‚
1. **Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach** (Arxiv 2025.02) [[paper]](http://arxiv.org/abs/2502.05171) æå‡ºä¸€ç§å¾ªç¯ç»“æ„æ¥æå‡reasoningèƒ½åŠ›ï¼šç±»ä¼¼RNNï¼Œå¾ªç¯ç»“æ„çš„æ¯ä¸€ä¸ªå¾ªç¯å—éƒ½æ¥å—åŸå§‹promptå’Œä¸Šä¸€ä¸ªçŠ¶æ€ä½œä¸ºè¾“å…¥ï¼›å¾ªç¯è¶Šå¤šæ€§èƒ½è¶Šå¥½ã€‚
1. **SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs** (Arxiv 2025.02) [[paper]](http://arxiv.org/abs/2502.12134) ç”¨ä¸€ä¸ªå°ç½‘ç»œæœ€åä¸€å±‚çš„éšå±‚è¡¨ç¤ºæ¥ä¸Šä¸€ä¸ªprojectorå¾—åˆ°æ‰€è°“çš„soft thoughtsï¼Œå°†ä¹‹ä¸é—®é¢˜æ–‡æœ¬ä¸€åŒè¾“å…¥ï¼Œåç»­è®©åšæ–‡æœ¬CoTã€‚ä¸ç”¨åƒCOCONUTé‚£æ ·fine-tuneæ•´ä¸ªLLMï¼Œé¿å…äº†ç¾éš¾æ€§é—å¿˜å¯¼è‡´çš„æ‰ç‚¹ã€‚ä½†æ˜¯æå‡ä¹Ÿæ¯”è¾ƒæœ‰é™ï¼Œæœ‰ç‚¹åƒä¸€ä¸ªç®€å•çš„prompt tuning + CoTã€‚

### 2024

1. **DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models** (Arxiv April 2024) æå‡ºGRPO ï¼ˆGroup Relative Policy Optimizationï¼‰

2. **Training Large Language Model to Reason in a Continuous Latent Space** (COCONUT Arxiv Dec 2024, ICLR 2025 è¢«æ‹’ï¼Œä¸»è¦æ˜¯å› ä¸ºç›¸æ¯”äºæ™®é€šCoTä¼šåœ¨GSM8Kä¸Šæ‰ç‚¹) [[paper]](https://openreview.net/forum?id=tG4SgayTtk) å°†reasoning stepçš„æŸäº›ä¸­é—´æ­¥ä»word embedding æ›¿æ¢ä¸ºè¯¥tokençš„last hidden stateã€‚ 

   



## Test-time compute

### 2024

1. **Scaling LLM Test-time Compute Optimally can be More Effective than Scaling Model Parameters**  [[paper]](https://arxiv.org/pdf/2408.03314) ç ”ç©¶äº†ä¸¤ç§scaling test-time computeçš„ç­–ç•¥ï¼š1ï¼‰åŸºäºverifierï¼ˆprocess reward modelï¼‰çš„ï¼›2ï¼‰åŸºäºæ¨¡å‹çš„self-revisionçš„ã€‚å‘ç°äº†æ ¹æ®å…·ä½“ä»»åŠ¡ï¼ˆä¸åŒéš¾åº¦ï¼‰æ¥é€‰æ‹©æœ€ä¼˜scalingç­–ç•¥èƒ½åœ¨è¾¾åˆ°ç›¸åŒæ€§èƒ½æ—¶ç›¸æ¯”best-of-Né™ä½å››å€è®¡ç®—é‡



## Alignment

### 2024

1. **LETâ€™S VERIFY STEP BY STEP** (ICLR 2024) å‘ç°PRMæ¯”ORMå¥½
1. **The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning** (ICLR 2024) [[paper]](https://openreview.net/forum?id=wxJ0eXwwda) é€šè¿‡ICLï¼Œæ·»åŠ system promptå’Œé£æ ¼åŒ–çš„è¾“å‡ºï¼Œå®ç°åªç”¨å¾ˆå°‘çš„æ ·æœ¬ï¼ˆ3ä¸ªï¼‰æ¥æå‡LLM alignmentã€‚
1. **The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2404.13208) æ„é€ è®­ç»ƒæ•°æ®æ¥æ•™æ¨¡å‹å­¦ä¹ ä¸åŒæŒ‡ä»¤çš„ä¼˜å…ˆçº§æ¥é˜²å¾¡æœ‰å®³æŒ‡ä»¤ã€‚å…·ä½“æ–¹æ³•ä¸ºï¼Œå¯¹äºä¸åŒçš„ä»»åŠ¡ï¼Œåˆ†åˆ«æ„é€ ä¸æœ€é«˜æŒ‡ä»¤aligned/misalignedçš„æŒ‡ä»¤ï¼Œç„¶åè®­ç»ƒæ¨¡å‹è¾“å‡ºæœŸæœ›çš„å›ç­”ã€‚

### 2023

1. **(DPO) Direct Preference Optimization: Your Language Model is Secretly a Reward Model** (NeurIPS 2023) [[paper]](http://arxiv.org/abs/2305.18290)

### 2017

1. **(RLHF) Deep reinforcement learning from human preferences** (NeurIPS 2017) [[paper]](http://arxiv.org/abs/1706.03741)
2. **(PPO) Proximal Policy Optimization Algorithms** (Arxiv 2017) [[paper]](http://arxiv.org/abs/1707.06347)



## Interpretability

### 2024

1. **LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations** (ICLR 2025 Ratings:8666) [[paper]](https://openreview.net/forum?id=KRnsX5Em3W) ç”¨ä¸€ä¸ªçº¿æ€§probeæ¥æ ¹æ®æ¨¡å‹ä¸­é—´å±‚è¡¨ç¤ºåˆ¤æ–­æ¨¡å‹è¾“å‡ºçš„æ­£ç¡®ä¸å¦ã€‚ç„¶åè®©LLMå¯¹åŒä¸€ä¸ªé—®é¢˜ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œå¹¶ç”¨è¯¥åˆ†ç±»å™¨ç­›é€‰å‡ºæ­£ç¡®æ¦‚ç‡æœ€é«˜çš„ç­”æ¡ˆï¼Œå‘ç°èƒ½ç›¸æ¯”åŸæœ¬çš„ç­”æ¡ˆæ­£ç¡®ç‡æ›´é«˜ã€‚
1. **Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell** (EMNLP 2024 Findings) [[paper] ](http://arxiv.org/abs/2406.14673)ç”¨ä¸€ä¸ªçº¿æ€§probeæ¥æ ¹æ®æ¨¡å‹ä¸­é—´å±‚è¡¨ç¤ºæ¥ç›´æ¥é¢„æµ‹é—®é¢˜çš„ç­”æ¡ˆã€‚å‘ç°probe accæ¯”ç›´æ¥ç”Ÿæˆçš„accå¥½ã€‚
1. **Does Representation Matter? Exploring Intermediate Layers in Large Language Models** (NeurIPS 2024 workshop) [[paper]](http://arxiv.org/abs/2412.09563) LLMçš„ä¸­é—´å±‚ä¸‹æ¸¸æ€§èƒ½æ¯”æœ€åä¸€å±‚å¥½ã€‚æ¢ç©¶äº†Prompt Entropyã€Curvatureç­‰representation qualityçš„æŒ‡æ ‡å’Œä¸‹æ¸¸accçš„å…³ç³»ã€‚
1. 



## Other

### 2024

1. **Model Editing with Canonical Examples** [[paper]](http://arxiv.org/abs/2402.06155) æå‡ºäº†ä¸€ä¸ªæ–°ä»»åŠ¡ï¼šè®©æ¨¡å‹å­¦ä¹ å‡ ä¸ªç‰¹å®šçš„æ–‡æœ¬ä¾‹å­ï¼Œä»¥å®ç°æŸäº›çº æ­£ï¼ŒåŒæ—¶è¿˜ä¸èƒ½è®©æ¨¡å‹æ”¹å˜å¾ˆå¤šã€‚

1. **Evaluating Large Language Models at Evaluating Instruction Following** [[paper]](https://openreview.net/forum?id=tr0KidwPLc) (ICLR 2024) 

1. **Not all Layers of LLMs are Necessary during Inference** (Arxiv April 2024) è®­ç»ƒä¸€ä¸ªå¯¹LLMä¸­é—´å±‚featureçš„åˆ†ç±»å™¨åˆ¤æ–­æ˜¯å¦åº”è¯¥æ—©åœæ¥è·å–æ—©åœå±‚æ•°ï¼Œæ¥åŠ é€ŸLLMæ¨ç†ã€‚è¿˜å‘ç°ä¸­é—´å±‚é¢„æµ‹çš„top probå’Œtop prob-second top probåœ¨å„ä¸ªä»»åŠ¡ä¸Šéƒ½å‘ˆç°å‡ºéšç€å±‚æ•°åŠ æ·±è€Œå¢åŠ å¹¶é€æ¸ç¨³å®šçš„è¶‹åŠ¿ï¼ˆä½†åœ¨ä¸åŒä»»åŠ¡ä¸Šå±‚æ•°ä¸ä¸€æ ·ï¼‰ã€‚[[paper]](http://arxiv.org/abs/2403.02181)

1. **Demonstrating Mutual Reinforcement Effect through Information Flow** (Arxiv March 2024) [[paper]](https://arxiv.org/pdf/2403.02902) ç ”ç©¶äº†åŒæ—¶è¿›è¡Œwordåˆ†ç±»å’Œtextåˆ†ç±»çš„MREï¼ˆMutual Reinforcement Effectï¼‰ä»»åŠ¡ï¼Œä¹Ÿè§‚å¯Ÿåˆ°äº†anchoré‚£ç¯‡ä¸­çš„ä¸‰ç§attention activationéšlayerçš„åˆ†å¸ƒè¶‹åŠ¿ã€‚

1. **A Theoretical Understanding of Self-Correction through In-context Alignment** (Arxiv May 2024) [[paper]](http://arxiv.org/abs/2405.18634) ç†è®ºåˆ†ætransformerä¸­çš„å„ä¸ªæ¨¡å—åœ¨self-correctionä¸­å‘æŒ¥çš„ä½œç”¨

1. **Mechanics of Next Token Prediction with Self-Attention** (AISTATS 2024) [[paper]](https://proceedings.mlr.press/v238/li24f.html) æ„é€ äº†ä¸€ä¸ªgraphæ¥æè¿°next token predictionä»»åŠ¡ï¼Œåœ¨ç®€åŒ–settingä¸‹ç†è®ºåˆ†æå‡ºlast tokenæ›´å€¾å‘äºç»™æ›´ç»å¸¸ä½œä¸ºlabelçš„tokenåˆ†é…æ›´é«˜çš„attentionã€‚

1. **The pitfalls of next-token prediction** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2403.06963) æŒ‡å‡ºäº†è‡ªå›å½’æ¨¡å‹çš„ç¼ºé™·ï¼šé”™è¯¯æ»šé›ªçƒæ•ˆåº”å’Œåœ¨ä¸€ä¸ªå•ä¸€tokenè·¯å¾„ä¸Šåªèƒ½å­¦å‡ºä¸€ä¸ªç±»ä¼¼induction headçš„shortcutæ¨¡å‹

1. **A Law of Next-Token Prediction in Large Language Models** (Arxiv Aug 2024) [[paper]](https://arxiv.org/pdf/2408.13442v1)

1. **SEMIEVOL: Semi-supervised Fine-tuning for LLM Adaptation** (Arxiv Oct 2024) [[paper]](https://arxiv.org/pdf/2410.14745) æå‡ºäº†åŠç›‘ç£fine-tuningæ¡†æ¶SEMIEVOLã€‚

   

### 2023

1. **Instruction-following Evaluation through Verbalizer Manipulation** (Arxiv July 2023) [[paper]](http://arxiv.org/abs/2307.10558) å‘ç°LLMéµå¾ªflipped-label instructionsçš„èƒ½åŠ›å¾ˆå·®ï¼Œè¯´æ˜ICLå¯èƒ½åªæ˜¯ç›´æ¥åˆ©ç”¨äº†é¢„è®­ç»ƒè¯­æ–™çš„çŸ¥è¯†ï¼Œè€Œä¸æ˜¯å­¦ä¹ äº†contextã€‚å³ä½¿æ˜¯å¼ºå¦‚GPT-4çš„æ¨¡å‹ä¹Ÿä¸èƒ½å¾ˆå¥½åœ°éµå¾ªflipped-label instructionsã€‚
2. **Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks** (Arxiv Aug 2023) [[paper]](http://arxiv.org/abs/2307.02477) ä¸€äº›ä¸»è¦å‘ç°ï¼šâ‘ æ¨¡å‹åœ¨counterfactualçš„settingä¸­æ€§èƒ½ä¼šå˜å·®ï¼Œä¸”settingå’Œå¸¸è§çš„ã€ç¬¦åˆäº‹å®çš„settingç›¸å·®è¶Šè¿œï¼Œæ€§èƒ½è¶Šå·®ï¼Œè¯´æ˜äº†æ¨¡å‹å¯èƒ½çš„è®°å¿†ç°è±¡ã€‚â‘¡åœ¨ç®—æœ¯ä»»åŠ¡ä¸Šï¼ŒICLèƒ½æå‡counterfactualï¼ˆä¸åŒè¿›åˆ¶çš„è®¡ç®—ï¼‰æ€§èƒ½ï¼Œä½†å’Œdefault settingçš„å·®è·éš¾ä»¥æŠ¹å¹³ã€‚
3. **Can the Inference Logic of Large Language Models be Disentangled into Symbolic Concepts?** (Arxiv Apr 2023) [[paper]](https://arxiv.org/abs/2304.01083) æå‡ºäº†ä¸€ç§empiricalçš„æŒ‡æ ‡æ¥è¡¡é‡è¾“å…¥å¥å­é‡Œçš„æŸäº›è¯å’Œè¯ç»„å¯¹æŸä¸€ç‰¹å®šè¾“å‡ºçš„å†³å®šç¨‹åº¦ã€‚
4. **Contrastive Chain-of-Thought Prompting** (Arxiv Nov 2023) [[paper]](http://arxiv.org/abs/2311.09277) ä½¿ç”¨å¯¹æ¯”CoTï¼Œå³ä¸€ä¸ªæ­£ç¡®CoTæ­é…ä¸€ä¸ªé”™è¯¯CoTèƒ½ç›¸æ¯”å¸¸è§„çš„CoTå¸¦æ¥æå‡.

### 2022

1. **Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models** [[paper]](https://arxiv.org/pdf/2210.14199.pdf)

### 2021

**LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS** å°†å¯¹æ¨¡å‹æƒé‡çŸ©é˜µçš„æ›´æ–°é™åˆ¶ä¸ºä½ç§©çŸ©é˜µä¹˜ç§¯$BA$çš„å½¢å¼ï¼Œæå¤§å‡å°‘äº†pre-trained modelè¿ç§»åˆ°æ–°ä»»åŠ¡çš„ä»£ä»·ï¼ˆä¸ç”¨fine-tuneæ‰€æœ‰å‚æ•°ï¼‰ [[paper]](https://arxiv.org/abs/2106.09685)

### 2019

1. **Are Sixteen Heads Really Better than One?** (NeurIPS 2019) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html) åœ¨æŸäº›å±‚ä¸Šï¼Œåªç”¨ä¸€ä¸ªheadæ€§èƒ½ä¹Ÿèƒ½ä¿æŒä¸å˜ã€‚åŒæ—¶æå‡ºäº†ä½¿ç”¨attentionæ¢¯åº¦æ¥è¡¡é‡headçš„é‡è¦æ€§ï¼Œæå‡ºäº†å‰ªæç­–ç•¥ã€‚



# VLM

## Evaluation and Understandings of Multimodal Reasoning

### 2025

1. **Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark** (Arxiv Jan 2025) [[paper]](http://arxiv.org/abs/2501.05444) ä¸€ä¸ªæ¯”è¾ƒå…¨é¢çš„æ¶µç›–æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ä»£ç çš„è§†è§‰æ¨ç†ä»»åŠ¡çš„benchmarkã€‚å‘ç°æ–‡æœ¬CoTå¾ˆéš¾æå‡2Då˜æ¢è¿™ç§éœ€è¦ç©ºé—´æƒ³è±¡çš„ä»»åŠ¡çš„æ€§èƒ½ã€‚

### 2024

1. **Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models** (NeurIPS 2024) [[paper]](http://arxiv.org/abs/2406.14852) 
   åœ¨ä¸‰ä¸ªåˆæˆçš„ç©ºé—´ç†è§£ä»»åŠ¡ä¸Šè¯„æµ‹LLMå’ŒLVMï¼Œä¸»è¦å‘ç°ï¼š1ï¼‰è¯¥ä»»åŠ¡çš„æ€»ä½“è¡¨ç°å¹¶ä¸å¥½ 2ï¼‰å¯¹äºVLMè€Œè¨€ï¼Œæ›´ä¾èµ–äºè¯­è¨€ä¿¡æ¯è€Œä¸æ˜¯è§†è§‰ä¿¡æ¯åšå†³ç­–ï¼Œå»æ‰/æ‰°ä¹±è§†è§‰ä¿¡æ¯ç”šè‡³ä¼šæœ‰æå‡ 3ï¼‰VLMä¸­çš„language encoderæ¯”åŒæ ·çš„å•ç‹¬LLMæ€§èƒ½æ›´å¥½ï¼Œè¯´æ˜å¤šæ¨¡æ€pretrainå¯¹äºlanguageæœ‰ç”¨ã€‚ã€insightã€‘ç°æœ‰çš„å°†è§†è§‰ä¿¡æ¯è½¬åŒ–åˆ°language spaceå†è¿›è¡Œæ¨ç†çš„èŒƒå¼ä¸å¤Ÿå¥½ã€‚
2. **Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?** (Arxiv Sep 2024) [[paper]](https://arxiv.org/abs/2406.02537) 
3. **TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners** (Arxiv June 2024) [[paper]](http://arxiv.org/abs/2406.02537) æäº†ä¸€ä¸ªæ–°çš„ä¿¯è§†å›¾ç†è§£çš„æ•°æ®é›†ï¼Œå‘ç°VLMçš„ä¿¯è§†å›¾ç†è§£èƒ½åŠ›ä»ç„¶å¾ˆå·®
4. **Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models** [[paper]](https://openreview.net/pdf?id=nFU4xCyoe0) åŸå­è§†è§‰ä»»åŠ¡benchmark Atomic Visual Skills Benchmark (AVSBench) 
5. **DOES SPATIAL COGNITION EMERGE IN FRONTIER MODELS? ** (Arxiv Oct 2024) [[paper]](http://arxiv.org/abs/2410.06468) æå‡ºäº†ç©ºé—´ç†è§£ä»»åŠ¡ SPACE benchmarkã€‚å‘ç°ç›®å‰æœ€å¼ºçš„æ¨¡å‹åœ¨ç®€å•çš„ç©ºé—´ä»»åŠ¡ä¸Šæ€§èƒ½å¾ˆå·®
6. **Towards Interpreting Visual Information Processing in Vision-Language Models** (ICLR 2025 886) æ£€æŸ¥ç‰©ä½“ä¿¡æ¯æ˜¯å¦ç¼–ç åœ¨äº†ç‰¹å®šçš„vision tokené‡Œã€‚å‘ç°object tokenå»æ‰ä¹‹åæ¨¡å‹æ‰ç‚¹æœ€ä¸¥é‡ã€‚é«˜gradient tokenå½±å“ä¹ŸæŒºå¤§ã€‚
7. **Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis**



## Improving Multimodal Reasoning

### 2025

1. **Imagine while Reasoning in Space: Multimodal Visualization-of-Thought** (Arxiv 2025.01) [[paper]](10.48550/arXiv.2501.07542) åˆ©ç”¨Anole-7bè¿™ç§èƒ½åŒæ—¶ç”Ÿæˆå›¾ç‰‡å’Œæ–‡å­—çš„æ¨¡å‹ï¼Œæ¯ä¸€æ­¥ç”Ÿæˆå›¾ç‰‡å’Œæ–‡æœ¬ï¼Œæ„æˆMultimodal Visualization-of-Thoughtï¼Œæå‡ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚åªåœ¨2dç½‘æ ¼è§†è§‰ä»»åŠ¡è¿›è¡Œäº†æµ‹è¯•ã€‚
1. **Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking** (Arxiv 2025.02) [[paper]](http://arxiv.org/abs/2502.02339) training-freeã€‚å®šä¹‰ä¸€ä¸ªåŠ¨ä½œç©ºé—´ï¼ˆVisual Parsingã€CoTã€divide-and-conquerç­‰ï¼‰åœ¨ä¸€ä¸ª500æ ·æœ¬çš„å°æ•°æ®é›†ä¸Šäº§ç”Ÿreasoning pathï¼Œä¸ºæ¯ä¸ªé—®é¢˜è¿›è¡ŒMCTSï¼šæ¯ä¸€æ­¥ä»åŠ¨ä½œç©ºé—´é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚ä¸ºæ¯ä¸ªé—®é¢˜å¾—åˆ°æœ€ä¼˜æ¨ç†è·¯å¾„åï¼Œä¸ºæ¯ä¸ªè·¯å¾„è®¡ç®—Problem Condition Complexity (PCC)ï¼Œæ¯ä¸ªé—®é¢˜-è·¯å¾„-PCCç§°ä¸ºä¸€ä¸ªcardã€‚æµ‹è¯•æ—¶ï¼Œè®¡ç®—æµ‹è¯•é—®é¢˜çš„PCCï¼Œå¹¶æ‰¾å‡ºä¸ä¹‹PCCæœ€æ¥è¿‘çš„cardï¼Œè®©å…¶æŒ‰ç…§è¿™ä¸ªcardçš„æ¯ä¸€æ­¥çš„actioné€‰æ‹©è¿›è¡Œæ¨ç†ã€‚è¿™æ ·é¿å…äº†æµ‹è¯•æ—¶è¿›è¡Œå¤æ‚çš„æœç´¢ã€‚
1. **Virgo: A Preliminary Exploration on Reproducing o1-like MLLM** (Arxiv 2025.02) [[paper]](http://arxiv.org/abs/2501.01904) ç”¨å°‘é‡ï¼ˆ5kï¼‰çº¯æ–‡æœ¬çš„long thoughtæ•°æ®è®­ç»ƒMLLMå°±èƒ½å¸¦æ¥æ˜¾è‘—æå‡
1. **URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics** (Arxiv 2025.02) [[paper]](http://arxiv.org/abs/2501.04686) å€ŸåŠ©GeminiåˆæˆCoTåšfine-tuneã€‚æäº†ä¸¤ç§æ–¹æ³•å¯¹SFTå¾—åˆ°çš„æ¨¡å‹è¿›ä¸€æ­¥è®­ç»ƒå¾—åˆ°ä¸€ä¸ªverifierï¼Œæ²¡å¤ªçœ‹æ‡‚æ–‡ä¸­æåˆ°çš„MCTSç”¨åœ¨å“ªäº†ä»¥åŠæ‰€æçš„MIEä¸ºä»€ä¹ˆèƒ½å¢å¼ºvisual perceptionèƒ½åŠ›ã€‚
1. 

### 2024

1. **Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models** (NeurIPS 2024) [[paper]](http://arxiv.org/abs/2406.09403) è®©æ¨¡å‹ç”Ÿæˆä»£ç æ¥è°ƒç”¨å·¥å…·æ ¹æ®ç°æœ‰çš„è§†è§‰è¾“å…¥äº§ç”Ÿæ–°çš„è§†è§‰å›¾åƒæ¥ä½œä¸ºæ¨ç†çš„è¾…åŠ©ï¼Œå¯ä»¥æå‡åœ¨å„ç§è§†è§‰ç›¸å…³ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚
2. **Task Navigator: Decomposing Complex Tasks for Multimodal Large Language Models** (CVPR 2024) [[paper]](https://openaccess.thecvf.com/content/CVPR2024W/MAR/papers/Ma_Task_Navigator_Decomposing_Complex_Tasks_for_Multimodal_Large_Language_Models_CVPRW_2024_paper.pdf) å·¥ç¨‹æ–‡ç« ï¼Œå€ŸåŠ©LLMæ ¹æ®å†å²å­é—®é¢˜å’Œæ¨¡å‹å›ç­”ï¼Œè¿­ä»£äº§ç”Ÿå¤šä¸ªå­é—®é¢˜ï¼Œæå‡MLLMå®Œæˆå¤æ‚è§†è§‰ç†è§£ä»»åŠ¡çš„èƒ½åŠ›ã€‚æå‡ºäº†VersaChallenge benchmarkï¼ŒåŒ…æ‹¬å¸¸è¯†æ¨ç†ã€ç‰©ç†å…³ç³»æ¨ç†ã€æœªæ¥é¢„æµ‹ç­‰ã€‚
3. **SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities** (CVPR 2024) [[paper]](https://ieeexplore.ieee.org/document/10658310/) æ„å»ºæ•°æ®é›†ï¼Œè®­äº†ä¸€ä¸ªspatial-VLMç”¨ä»¥è§£å†³ç©ºé—´ä»»åŠ¡
4. **SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models** (NeurIPS 2024) [[paper]](http://arxiv.org/abs/2406.01584) æ„å»ºç©ºé—´ä½ç½®å…³ç³»æ•°æ®é›†ï¼Œæ·»åŠ äº†ä¸€ä¸ªæ·±åº¦å›¾->è¯­è¨€æ¨¡å—ï¼Œæ¥å¢å¼ºå‡ ä½•æ¨ç†
5. **Multimodal Chain-of-Thought Reasoning in Language Models** (TMLR 2024) [[paper]](http://arxiv.org/abs/2302.00923) ä¸¤é˜¶æ®µè®­ç»ƒï¼Œç¬¬ä¸€é˜¶æ®µæ¥å—æ–‡æœ¬å’Œè§†è§‰çš„èåˆç‰¹å¾è¾“å‡ºä¸€ä¸ªrationaleï¼ˆæ¨ç†è¿‡ç¨‹çš„æ–‡æœ¬æè¿°ï¼‰ï¼Œç¬¬äºŒé˜¶æ®µå°†ç”Ÿæˆçš„rationaleå’ŒåŸå§‹æ–‡æœ¬ç»“åˆï¼Œå†ä¸è§†è§‰ç‰¹å¾èåˆé‡æ–°è¾“å…¥æ¨¡å‹äº§ç”Ÿé¢„æµ‹ã€‚
6. **Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination** (Arxiv Nov 2024) [[paper]](http://arxiv.org/abs/2411.12591) å¯¹äºVQAä»»åŠ¡ï¼Œæå‡ºthinking-before-lookingèŒƒå¼ï¼Œå…ˆåˆ©ç”¨ä¸€ä¸ªLLMæ ¹æ®æ–‡æœ¬é—®é¢˜ç”Ÿæˆä¸€å †æ›´ç»†è‡´çš„é—®é¢˜ï¼Œç„¶åå°†è¿™äº›é—®é¢˜å’Œå›¾ç‰‡ä¸€èµ·è¾“ç»™MLLMè®©å…¶ç”Ÿæˆæ¨ç†æ­¥éª¤ã€‚æœ€ç»ˆå°†åŸå§‹é—®é¢˜ã€å›¾ç‰‡ã€æ¨ç†æ­¥éª¤ä¸€èµ·è¾“ç»™MLLMè®©å…¶ç”Ÿæˆç­”æ¡ˆã€‚
7. **Link-Context Learning for Multimodal LLMs** (CVPR 2024) [[paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html) æå‡ºä¸€ç§æ–°çš„fine-tune MLLMçš„æ–¹æ³•ï¼šè®©contextå’Œqueryå…·æœ‰ä¸€å®šçš„causalè”ç³»ï¼Œå‘ç°èƒ½æå‡æ¨¡å‹é€šè¿‡contextå­¦ä¹ æ–°æ¦‚å¿µçš„èƒ½åŠ›
8. **Lever LM: Configuring In-Context Sequence to Lever Large Vision Language Models** (NeurIPS 2024) [[paper]](http://arxiv.org/abs/2312.10104) å…ˆæ„å»ºä¸€ä¸ªä¼˜è´¨çš„ICLæ•°æ®é›†ï¼Œç„¶åå°†è¯¥æ•°æ®é›†ä¸­çš„image-textå¯¹è§†ä½œtokenï¼Œç”¨CLIPæŠ½å–ç‰¹å¾ä½œä¸ºtoken embeddingï¼Œè®­ç»ƒä¸€ä¸ªå¾ˆå°çš„Transformerï¼ˆlever-LMï¼‰æ¥åœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œnext-token predictionï¼ˆåºåˆ—æ˜¯ä»queryåˆ°contextè¿™æ ·å€’ç€æ¥çš„ï¼‰ã€‚æµ‹è¯•æ—¶ï¼Œæœ€åç»™å®šæµ‹è¯•æ ·æœ¬ï¼Œæ‹¿lever-LMä»è¯¥é¢„å…ˆæŒ‘é€‰å¥½çš„æ•°æ®é›†ä¸­é¢„æµ‹åç»­çš„exampleæ¥æ„æˆcontextã€‚
9. **Natural Language Inference Improves Compositionality in Vision-Language Models** (ICLR 2025 Ratings 8866) [[paper]](https://openreview.net/forum?id=G3aXjVAJjU) promptå·¥ç¨‹ã€‚ä»»åŠ¡æ˜¯åˆ¤æ–­captionå’Œå›¾ç‰‡ç›¸ä¸ç›¸ç¬¦ã€‚åšæ³•æ˜¯è®©LLMç”Ÿæˆä¸åŸå§‹captionç›¸ç¬¦ã€ä¸ç›¸ç¬¦çš„yes or noé—®é¢˜ï¼Œç„¶åæ ¹æ®VLMåœ¨ç›¸ç¬¦/ä¸ç›¸ç¬¦/åŸå§‹é—®é¢˜ä¸Šçš„logitæ¥åšå‡ºæœ€ç»ˆåˆ¤æ–­ã€‚
10. **MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs** ï¼ˆICLR 2025) [[paper]](https://openreview.net/forum?id=DgaY5mDdmT) å‘ç°MLLMåœ¨object identificationä»»åŠ¡ä¸­èƒ½å¤Ÿå…³æ³¨åˆ°æ­£ç¡®çš„è§†è§‰åŒºåŸŸï¼Œå³ä½¿å›ç­”é”™è¯¯ã€‚æå‡ºäº†å‡ ä¸ªè‡ªåŠ¨åŒ–çš„training-freeçš„è£å‰ªå‡ºç›®æ ‡åŒºåŸŸçš„æ–¹æ³•ã€‚å°†ç›®æ ‡åŒºåŸŸçš„visual tokenè¿æ¥åˆ°åŸå§‹å›¾ç‰‡tokenåé¢ã€‚
11. **Interleaved-Modal Chain-of-Thought** (Arxiv 2024.11) [[paper]](https://arxiv.org/pdf/2411.19488) åœ¨æ¯ä¸€ä¸ªreasoning stepé€‰å‡ºattentionæœ€é«˜çš„visual tokensï¼Œä¿æŒåŸå›¾çš„é¡ºåºæ’å…¥åˆ°è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ä¹‹åã€æ–‡æœ¬rationaleå¼€å§‹ä¹‹å‰çš„ä½ç½®ï¼Œä¹‹åå†æ®æ­¤ç”Ÿæˆrationaleã€‚æŒ‰æ­¤æ–¹æ³•è¿­ä»£ç”Ÿæˆå¤šä¸ªreasoning stepï¼Œç„¶åå†åœ¨å…¶åç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
12. **Progressive Multimodal Reasoning via Active Retrieval** (Arxiv 2024.12) [[paper]](Progressive Multimodal Reasoning via Active Retrieval) æå‡ºäº†ä¸€ä¸ªä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ ¹æ®å½“å‰æ¨ç†æ­¥æœç´¢ç›¸å…³çŸ¥è¯†ï¼Œå¹¶é€šè¿‡MCTSæ¥æ„å»ºCoTçš„æ¡†æ¶ï¼Œå¹¶æå‡ºäº†åœ¨ç”Ÿæˆçš„CoTæ•°æ®ä¸Šè¿›è¡ŒPRMçš„æ–¹æ³•ã€‚æ¨ç†æ—¶æ ¹æ®PRMçš„æ‰“åˆ†ï¼Œé€‰å–å¾—åˆ†topké«˜çš„æ¨ç†è·¯å¾„ã€‚
13. **Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search** (Arxiv 2024.12) [[paper]](Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search) [[code]](https://github.com/HJYao00/Mulberry) ç”¨MCTSæ„å»ºCoTï¼Œå…¶ä¸­æ¯ä¸€æ­¥æ‰“åˆ†åˆ©ç”¨å¤šä¸ªæ¨¡å‹ï¼›åŒæ—¶æ„å»ºåæ€é“¾ï¼Œåšæ³•æ˜¯æ„å»ºä¸€ä¸ªâ€œä½å¾—åˆ†èŠ‚ç‚¹-åæ€prompt-é«˜å¾—åˆ†èŠ‚ç‚¹â€çš„æ€ç»´é“¾ã€‚ç„¶åç”¨ç”Ÿæˆçš„æ€»å…±260Kæ•°æ®è¿›è¡Œfine-tuneã€‚
14. **Perception Tokens Enhance Visual Reasoning in Multimodal Language Models** (Arxiv 2024.12) [[paper]](http://arxiv.org/abs/2412.03548) é’ˆå¯¹ç›¸å¯¹æ·±åº¦ä¼°è®¡é—®é¢˜æˆ–è®¡æ•°é—®é¢˜ï¼Œå°†æ·±åº¦å›¾æˆ–bounding boxè½¬æ¢ä¸ºMLLMèƒ½å¤„ç†çš„tokenæ¥æä¾›æ›´ç²¾ç»†çš„è§†è§‰ä¿¡æ¯ï¼Œå¹¶åŠ å…¥åˆ°CoTä¸­ï¼Œæ¥fine-tune MLLMã€‚
15. **MR-MLLM: Mutual Reinforcement of Multimodal Comprehension and Vision Perception** (Arxiv 2024.06) [[paper]](http://arxiv.org/abs/2406.15768)
16. **Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning** (NeurIPS 2024 DB track) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2024/file/0ff38d72a2e0aa6dbe42de83a17b2223-Paper-Datasets_and_Benchmarks_Track.pdf) é€ äº†ä¸€ä¸ªæ•°æ®é›†Visual CoTï¼ŒåŒ…å«æ¨ç†å…³é”®è§†è§‰åŒºåŸŸçš„bounding boxçš„åæ ‡ã€‚æå‡ºçš„æ–¹æ³•ï¼šè®­ç»ƒMLLMåœ¨æ¨ç†æ—¶è¾“å‡ºbounding boxã€‚
17. **Cantor: Inspiring Multimodal Chain-of-Thought of MLLM** (MM 2024) [[paper]](http://arxiv.org/abs/2404.16033) çº¯prompt engineeringæ–‡ç« ã€‚ä¸ºäº†å¢å¼ºperceptionï¼Œæç¤ºMLLMæ ¹æ®é—®é¢˜æ‰¾å‡ºå…·ä½“è¯¥çœ‹ä»€ä¹ˆå›¾ç‰‡ç»†èŠ‚ï¼Œç„¶åé—®ä¸€ä¸ªMLLMè®©å®ƒä¸“é—¨å»çœ‹ï¼Œæœ€åå†ç»¼åˆå®ƒçš„è¾“å‡ºæ¥åšæœ€ç»ˆå›ç­”

 ### 2023

1. **Multi-modal Latent Space Learning for Chain-of-Thought Reasoning in Language Models** (Arxiv 2023.12) [[paper]](http://arxiv.org/abs/2312.08762) è®¤ä¸ºCLIPçš„è§†è§‰ç‰¹å¾ä¸åˆ©äºCoTæ¨ç†ã€‚è®­ç»ƒä¸€ä¸ªdiffusion modelæ¥è·å–è§†è§‰ç‰¹å¾ã€‚



## Hallucination of VLMs

### 2025

1. **The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering** (Arxiv 2025.02) [[paper]](http://arxiv.org/abs/2502.03628) å‘ç°éšç€ç”Ÿæˆçš„è¿›è¡Œï¼Œå›¾ç‰‡ä¸­çœŸå®å‡ºç°çš„å…ƒç´ çš„tokenåœ¨logitä¸­çš„æ’åä¼šé€æ¸ä¸‹é™ï¼Œè€Œå¹»è§‰è¯çš„æ’åä¼šé€æ¸é å‰ã€‚æå‡ºäº†ä¸€ç§è¾ƒä¸ºå¯å‘å¼çš„ç±»ä¼¼task vectorçš„æ–¹æ³•æ¥ç¼“è§£ã€‚å®éªŒæ•ˆæœä¸Šä¸»è¦æ˜¯é™ä½å¹»è§‰ï¼Œè€Œä¸æ˜¯å¢å¼ºæ¨ç†ã€‚

### 2024

1. **Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination** (Arxiv Nov 2024) [[paper]](http://arxiv.org/abs/2411.12591) å¯¹äºVQAä»»åŠ¡ï¼Œæå‡ºthinking-before-lookingèŒƒå¼ï¼Œå…ˆåˆ©ç”¨ä¸€ä¸ªLLMæ ¹æ®æ–‡æœ¬é—®é¢˜ç”Ÿæˆä¸€å †æ›´ç»†è‡´çš„é—®é¢˜ï¼Œç„¶åå°†è¿™äº›é—®é¢˜å’Œå›¾ç‰‡ä¸€èµ·è¾“ç»™MLLMè®©å…¶ç”Ÿæˆæ¨ç†æ­¥éª¤ã€‚æœ€ç»ˆå°†åŸå§‹é—®é¢˜ã€å›¾ç‰‡ã€æ¨ç†æ­¥éª¤ä¸€èµ·è¾“ç»™MLLMè®©å…¶ç”Ÿæˆç­”æ¡ˆã€‚
2. **Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention** (ICLR 2025 8866) [[paper]](https://openreview.net/forum?id=Bjq4W7P2Us) å‘ç°å¹»è§‰çš„äº§ç”Ÿæ˜¯ç”±äºæŸäº›ç‰¹å®šçš„attention headï¼Œè¿™äº›headæ˜¯æºè‡ªVLMçš„LMéƒ¨åˆ†ã€‚ä»–ä»¬ä¼šç»™æ–‡æœ¬åˆ†é…æ›´é«˜çš„attentionã€‚æå‡ºäº†åœ¨æ¨ç†æ—¶å…³é—­è¿™äº›å¹»è§‰headå’Œåœ¨instruction tunningæ—¶ä¸“é—¨è°ƒè¿™äº›headä¸¤ç§æ”¹è¿›æ–¹æ³•ã€‚
3. **Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering** (ICLR 2025 886) [[paper]](https://openreview.net/forum?id=LBl7Hez0fF) åŠ¨æœºï¼šå‘ç°ä½¿ç”¨æ‰°åŠ¨åå†å¹³å‡çš„vision featureèƒ½é™ä½å¹»è§‰ï¼Œè®¤ä¸ºå¹»è§‰æ¥è‡ªvision encoderçš„ä¸å¤Ÿé²æ£’ã€‚æå‡ºä½¿ç”¨in-context vectorçš„åšæ³•ï¼Œè®¡ç®—ä»æ­£å¸¸featureåˆ°æ‰°åŠ¨å¹³å‡åçš„featureçš„ä¸»æˆåˆ†ï¼ŒåŠ åˆ°æ¨ç†çš„æ—¶å€™ã€‚
4. **Analyzing and Mitigating Object Hallucination in Large Vision-Language Models** (ICLR 2024) [[paper]](http://arxiv.org/abs/2310.00754) å‘ç°äº†å¹»è§‰äº§ç”Ÿçš„å‡ ä¸ªè§¦å‘å› ç´ ï¼š1)è®­ç»ƒæ•°æ®ä¸­çš„æŸä¸¤ç§å¯¹è±¡çš„spuriouså…±ç°å…³ç³» 2)decodingè¿‡ç¨‹çš„ä¸ç¡®å®šæ€§ä¼šå°†å¹»è§‰è¯é‡‡æ ·å‡ºæ¥ï¼ˆå³ä½¿å¹»è§‰è¯çš„ç”Ÿæˆæ¦‚ç‡æœ¬ä¸åº”è¯¥æ˜¯æœ€é«˜ï¼‰ 3)å¹»è§‰æ›´å®¹æ˜“å‡ºç°åœ¨ç”Ÿæˆæ–‡æœ¬ä¸­é åçš„ä½ç½®
5. **Debiasing Multimodal Large Language Models** (Arxiv Mar 2024) [[paper]](http://arxiv.org/abs/2403.05262) åŒæ ·å‘ç°äº†VLMå…³æ³¨text tokençš„é—®é¢˜ã€‚æå‡ºäº†ä¸¤ç§decodingçš„ç­–ç•¥ã€‚å…¶ä¸­ä¸€ç§ç±»ä¼¼Trusting Your Evidenceé‚£ç¯‡å¢å¼ºå¯¹äºcontextçš„å…³æ³¨çš„contrastive decodingæ–¹æ³•ï¼š $y=\text{softmax}((1+\alpha) p_\theta(y|v,x)-\alpha p_\theta(y|v',x))$ ï¼Œå…¶ä¸­ç¬¬ä¸€é¡¹å’Œç¬¬äºŒé¡¹åˆ†åˆ«è¡¨ç¤ºæ­£å¸¸çš„å›¾æ–‡è¾“å…¥å’Œä»…æ–‡æœ¬è¾“å…¥æ—¶çš„è¾“å‡ºã€‚
6. **IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding** (Arxiv Feb 2024) [[paper]](http://arxiv.org/abs/2402.18476) ä¹Ÿæå‡ºäº†contrastive decodingçš„æ–¹æ³•ï¼Œç”¨ä¸€ä¸ªæ›´åŠ å…³æ³¨è§†è§‰tokençš„æ¨¡å‹ $\hat{\theta}$ çš„logitå‡å»åŸå§‹æ¨¡å‹ $\theta$ çš„logitï¼Œè¯¥é¡¹ç§°ä¸ºCD scoreã€‚æ„å»ºâ€œæ›´åŠ å…³æ³¨è§†è§‰tokençš„æ¨¡å‹â€çš„æ–¹æ³•ï¼šå¢å¤§å¯¹è§†è§‰tokençš„attention scoreã€‚åŒæ—¶ä½¿ç”¨ä¸¤ä¸ªè‡ªé€‚åº”æƒé‡æ¥è°ƒèŠ‚è¯¥contrastive decodingçš„ç¨‹åº¦ï¼š1) $\hat{\theta}$ å’Œ $\theta$ çš„é¢„æµ‹è¶Šåƒï¼ŒCD scoreæƒé‡è¶Šå°ï¼›2) ç”±äºå‘ç°ç”Ÿæˆcontent tokenï¼ˆæœ‰å®é™…æ„ä¹‰çš„ï¼‰ç›¸æ¯”function tokenï¼ˆæ— å®é™…æ„ä¹‰çš„è¿è¯ç­‰ï¼‰çš„CD scoreæ›´å¤§ï¼Œä¹Ÿå°±æ˜¯è¯´æ›´åŠ å…³æ³¨imageåªå¯¹content tokençš„æ­£ç¡®ç”Ÿæˆæ›´æœ‰åˆ©ï¼Œæ‰€ä»¥å¯¹content tokenæ·»åŠ æ›´å¤§çš„æƒé‡ï¼Œè€Œå¯¹function tokenæ·»åŠ è¾ƒå°çš„æƒé‡ã€‚
7. **Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs** (ECCV 2024) [[paper]](https://arxiv.org/pdf/2407.21771) å‘ç°å½“å»æ‰å›¾åƒï¼Œä¸”è®©æ¨¡å‹åœ¨å…¶åœ¨æœ‰å›¾åƒçš„æƒ…å†µä¸‹æ‰€ç”Ÿæˆçš„æ–‡æœ¬çš„åŸºç¡€ä¸Šç»§ç»­ç”Ÿæˆï¼Œä»ç„¶ä¼šå‡ºç°ç›¸åŒçš„å¹»è§‰ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºtext inertiaï¼ˆæ–‡æœ¬æƒ¯æ€§ï¼‰å¹»è§‰ã€‚æå‡ºçš„æ–¹æ³•ä¹Ÿæ˜¯contrastive decodingï¼šç”¨æ­£å¸¸çš„predictionå‡å»çº¯æ–‡æœ¬çš„prediction
8. **Mitigating object hallucinations in large vision-language models through visual contrastive decoding** (CVPR 2024) Visual Contrastive Decoding (VCD)
9. **Mitigating hallucinations in large vision-language models with instruction contrastive decoding** (ACL Findings 2024) Instruction Contrastive Decoding (ICD)
10. **OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation** (CVPR 2024) [[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_OPERA_Alleviating_Hallucination_in_Multi-Modal_Large_Language_Models_via_Over-Trust_CVPR_2024_paper.pdf) å‘ç°ç”Ÿæˆå›ç­”ä¸­çš„summary tokenï¼ˆæŒ‡attnéƒ½é›†ä¸­åœ¨å…¶ä¸Šçš„tokenï¼Œä¸”å¾€å¾€æ˜¯æ— æ„ä¹‰tokenï¼Œæ— æ³•è•´å«ä¸°å¯Œçš„è§†è§‰ä¿¡æ¯ï¼‰è¶Šå¤šï¼Œå¹»è§‰è¶Šä¸¥é‡ã€‚æå‡ºäº†è¯†åˆ«ç”Ÿæˆtokenä¸­çš„summary tokenå¹¶æ®æ­¤å‡è½»å¹»è§‰çš„ç­–ç•¥
11. **Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models** (ICLR 2025 Ratings: 8665) [[paper]](http://arxiv.org/abs/2408.02032) é¦–å…ˆæŒ‡å‡ºäº†è¿‡å¾€çš„contrastive decodingæ–¹æ³•çš„é—®é¢˜ï¼šæœ‰å¯èƒ½æ‰€å‡å»çš„å¹»è§‰è¾“å‡ºâ€œä¸å¤Ÿå¹»è§‰â€ï¼Œå¯¼è‡´æ­£å¸¸è¾“å‡ºå‡å»å®ƒä¹‹ååè€Œä¸å‡†ç¡®äº†ã€‚æœ¬æ–‡è®¤ä¸ºä½attention scoreçš„vision tokenæ›´å®¹æ˜“å¯¼è‡´å¹»è§‰ï¼Œå› æ­¤ä¸ºäº†æ›´å¥½åœ°å¼•å‘å¹»è§‰è¾“å‡ºå†å‡å»å®ƒï¼Œæå‡ºåœ¨æ¨ç†æ—¶ä»…ä¿ç•™ä½attention scoreçš„tokenã€‚ 
12. **Intervening Anchor Token: Decoding Strategy in Alleviating Hallucinations for MLLMs** (ICLR 2025 Ratings: 8866) [[paper]](https://openreview.net/forum?id=zGb4WgCW5i) å…ˆå®šä¹‰äº†ä¸€ç§åˆ†æå·¥å…·ï¼štoken propagation probability $\rho$ ï¼Œæ¥æè¿°ä¸€ä¸ªtokenåœ¨å‰ä¼ æ—¶çš„è´¡çŒ®ã€‚å‘ç°å¹»è§‰å’Œ $\rho$ çš„ä½ç†µæœ‰å…³ï¼ˆattentionéƒ½é›†ä¸­åœ¨summary tokenä¸Šäº†ï¼Œä»è€Œä¸¢å¤±äº†è§†è§‰tokençš„ä¿¡æ¯ï¼‰ã€‚ç†è®ºè¯æ˜äº†å°†QKçŸ©é˜µçš„äºŒèŒƒæ•°æ§åˆ¶åœ¨ä¸€ä¸ªåˆç†èŒƒå›´å†…å¯ä»¥å¢å¤§ $\rho$ çš„ç†µï¼Œæäº†ä¸€ä¸ªå¯å‘å¼ç­–ç•¥æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚
13. **Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs** (ICLR 2025 Ratings: 8666) [[paper]](https://openreview.net/forum?id=3PRvlT8b1R) ç°æœ‰çš„è§£å†³å¹»è§‰çš„æ–¹æ³•éš¾ä»¥æå‡åœ¨è§†è§‰æ¨ç†benchmarkä¸Šçš„èƒ½åŠ›ã€‚VLMèƒ½è¯†åˆ«è§†è§‰å…ƒç´ ï¼Œä½†éš¾ä»¥åˆ©ç”¨å®ƒä»¬è¿›è¡Œæ¨ç†ã€‚



## Alignment

### 2025

1. **MM-RLHF: The Next Step Forward in Multimodal LLM Alignment** (Arxiv 2025.02) [[paper]](http://arxiv.org/abs/2502.10391) æå‡ºCritique-Based Reward Model, ä»¥åŠä¸€æ•´å¥—ä»æ”¶é›†æ•°æ®åˆ°laligenmtçš„pipelineã€‚



## Interpretability

### 2024 

1. **Towards Interpreting Visual Information Processing in Vision-language Models** (ICLR 2025 Ratings: 8866) å‘ç°object tokenï¼ˆå›¾åƒä¸­å¯¹åº”äºç‰©ä½“çš„tokenï¼‰å»æ‰ä¹‹åæ¨¡å‹æ‰ç‚¹æœ€ä¸¥é‡ã€‚ä¸”å‘ç°é˜»å¡object tokenåˆ°last tokençš„attentionä¹‹åæ‰ç‚¹æœ€ä¸¥é‡ã€‚è¯´æ˜åœ¨è¯†åˆ«ç‰©ä½“æ—¶ï¼Œä¿¡æ¯ç›´æ¥ä»object tokenä¼ é€’åˆ°last tokenã€‚
1. **Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey** (Arxiv Dec 2024) [[paper]](http://arxiv.org/abs/2412.02104) Survey



## Unifying Understanding and Generation

### 2024

1. **Emu3: Next-Token Prediction is All You Need** (Arxiv September 2024) [[paper]](http://arxiv.org/abs/2409.18869) å°†æ–‡æœ¬ã€å›¾ç‰‡ã€è§†é¢‘éƒ½è½¬åŒ–ä¸ºtokenï¼Œè¿›è¡Œnext-token predictionçš„é¢„è®­ç»ƒã€‚èƒ½åŒæ—¶åšå›¾ç‰‡è§†é¢‘çš„ç”Ÿæˆã€è§†è§‰-è¯­è¨€ç†è§£ã€‚è®­ç»ƒæ¨¡å‹ï¼šåŒ…å«æ–‡æœ¬encoderï¼ˆT5ï¼‰ã€è§†è§‰encoderï¼ˆViT-largeï¼‰å’Œæ–‡æœ¬decoderï¼ˆT5ï¼Œè¾“å…¥ä¸ºè§†è§‰-æ–‡æœ¬èåˆç‰¹å¾ï¼Œè¾“å‡ºä¸ºæ–‡æœ¬ï¼‰ã€‚è®­ç»ƒèµ„æºï¼š8*32G V100ã€‚
2. **Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation** (Arxiv Oct 2024) [[paper]](http://arxiv.org/abs/2410.13848) ç”¨ä¸€ä¸ªè‡ªå›å½’transformerç»Ÿä¸€å®ç°å¤šæ¨¡æ€çš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡



## Multimodal ICL

### 2024

1. **Link-Context Learning for Multimodal LLMs** (CVPR 2024) [[paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html) æå‡ºä¸€ç§æ–°çš„fine-tune MLLMçš„æ–¹æ³•ï¼šè®©contextå’Œqueryå…·æœ‰ä¸€å®šçš„causalè”ç³»ï¼Œå‘ç°èƒ½æå‡æ¨¡å‹é€šè¿‡contextå­¦ä¹ æ–°æ¦‚å¿µçš„èƒ½åŠ›
2. **Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?** (Arxiv Sep 2024) 
3. **Finding Visual Task Vectors** (ECCV 2024) [[paper]](https://arxiv.org/pdf/2404.05729) 
4. **Lever LM: Configuring In-Context Sequence to Lever Large Vision Language Models** (NeurIPS 2024) [[paper]](http://arxiv.org/abs/2312.10104) å…ˆæ„å»ºä¸€ä¸ªä¼˜è´¨çš„ICLæ•°æ®é›†ï¼Œç„¶åå°†è¯¥æ•°æ®é›†ä¸­çš„image-textå¯¹è§†ä½œtokenï¼Œç”¨CLIPæŠ½å–ç‰¹å¾ä½œä¸ºtoken embeddingï¼Œè®­ç»ƒä¸€ä¸ªå¾ˆå°çš„Transformerï¼ˆlever-LMï¼‰æ¥åœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œnext-token predictionï¼ˆåºåˆ—æ˜¯ä»queryåˆ°contextè¿™æ ·å€’ç€æ¥çš„ï¼‰ã€‚æµ‹è¯•æ—¶ï¼Œæœ€åç»™å®šæµ‹è¯•æ ·æœ¬ï¼Œæ‹¿lever-LMä»è¯¥é¢„å…ˆæŒ‘é€‰å¥½çš„æ•°æ®é›†ä¸­é¢„æµ‹åç»­çš„exampleæ¥æ„æˆcontextã€‚
5. **Towards Global Optimal Visual In-Context Learning Prompt Selection** (NeurIPS 2024) [[paper]](http://arxiv.org/abs/2405.15279) æ²¡ç»†çœ‹ï¼Œä¹Ÿæ˜¯åšICL exampleæ’åºçš„ã€‚base ideaéƒ½æ˜¯ä¸æµ‹è¯•æ ·æœ¬è¶Šç›¸ä¼¼çš„exampleæ•ˆæœè¶Šå¥½ã€‚è®­ç»ƒä¸€ä¸ªç”¨äºæ’åºçš„transformerè¿›è¡Œå±€éƒ¨æ’åºï¼Œå†æ ¹æ®å±€éƒ¨æ’åºè®­ç»ƒä¸€ä¸ªå…¨å±€æ’åºä¿¡æ¯çš„å‘é‡ã€‚
6. **What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration** (NeurIPS 2024) [[paper]](http://arxiv.org/abs/2410.20482) ä»demoé€‰æ‹©ã€demoé¡ºåºå’Œcontextçš„æ„å»ºä¸‰ä¸ªè§’åº¦æ¢ç©¶äº†å½±å“å¤šæ¨¡æ€ICLçš„å› ç´ 
7. **What Makes Multimodal In-Context Learning Work?** (CVPR 2024 Workshop on Prompting in Vision) [[paper]](https://arxiv.org/abs/2404.15736) å¯¹Multimodal ICLçš„å®éªŒæ€§åˆ†æï¼Œä¸»è¦å‘ç°ï¼šæ–‡æœ¬å’Œå›¾åƒåŒæ—¶è¾“å…¥æ—¶ï¼ŒMLLMæ›´ä¾èµ–æ–‡æœ¬ï¼›ç›®å‰çš„MICLåŸºæœ¬ä¸Šæ˜¯åœ¨åšä»context copy
8. **Task vectors are cross-modal** (ICLR 2025 submission) 

### 2023

1. **What Makes Good Examples for Visual In-Context Learning?** [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/398ae57ed4fda79d0781c65c926d667b-Paper-Conference.pdf) çº¯vision ICLã€‚æ‰¾å’Œqueryæœ€ç›¸è¿‘çš„æ ·æœ¬æ¥åšICLï¼Œç±»ä¼¼Link-context learningã€‚





## Prompt Learning

### Prompt learningï¼š

1. **Conditional Prompt Learning for Vision-Language Models** (CoCoOp, CVPR2022) å°†å›¾ç‰‡ç‰¹å¾ç›´æ¥åŠ åˆ°context tokenä¸Šï¼Œè·å¾—sample-wiseçš„promptï¼Œä»¥å®ç°instanceçš„generalizationã€‚å…¶å®å°±æ˜¯å¸Œæœ›é€šè¿‡å¼•å…¥å›¾åƒä¿¡æ¯æ¥ä½¿å¾—promptæè¿°å¾—æ›´è´´åˆ‡ã€‚ä¸è¿‡æ„Ÿè§‰è¿˜æ˜¯æœ‰ç‚¹æ€ªï¼Œå› ä¸ºæ‰€æœ‰classéƒ½åŠ ä¸Šäº†åŒæ ·çš„å¯å­¦ä¹ prefixï¼Œä¸ºä»€ä¹ˆèƒ½æé«˜é¢„æµ‹ä¸ºæ­£ç¡®ç±»çš„æ¦‚ç‡ï¼Ÿ
2. MaPLe: Multi-modal Prompt Learning, CVPR2023 
3. Prompt-aligned Gradient for Prompt Tuning, ICCV2023
4. Compound Text-Guided Prompt Tuning via Image-Adaptive Cues, AAAI2024
5. MmAP : Multi-modal Alignment Prompt for Cross-domain Multi-task Learning, AAAI2024
6. **Improving Zero-Shot Generalization for CLIP with Synthesized Prompts** (ICCV 2023)

### For DA:

1. Domain Adaptation via Prompt Learning, arxiv 2022
2. AD-CLIP: Adapting Domains in Prompt Space Using CLIP, ICCV2023
3. Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation, NIPS2023
4. Prompt-based Distribution Alignment for Unsupervised Domain Adaptation, AAAI2024

### For DG:

1. StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization, arxiv2023





## Other

### 2024

1. **VisionLLaMA: A Unified LLaMA Interface for Vision Tasks** (Arxiv Mar 2024) [[paper]](https://arxiv.org/pdf/2403.00522) Vision LLaMa
1. **Are We on the Right Way for Evaluating Large Vision-Language Models?** (Arxiv April 2024) [[paper]](http://arxiv.org/abs/2403.20330) ç°æœ‰çš„vision-languageæ•°æ®é›†è´¨é‡ä¸å¤Ÿå¥½ï¼Œå¾ˆå¤šé—®é¢˜éƒ½æ˜¯åªçœ‹è¯­è¨€éƒ¨åˆ†å°±èƒ½è§£å†³ï¼Œæˆ–è€…é—®é¢˜åœ¨ç±»ä¼¼çš„è®­ç»ƒè¯­æ–™ä¸­è§è¿‡ï¼Œæ ¹æœ¬ä¸éœ€è¦å›¾ç‰‡ï¼›æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„vision-languageæ•°æ®é›†ã€‚
   1. **Visual Instruction Tuning** (NeurIPS 2023) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf) LLaVA
